# ===================================================================================
# æ–‡ä»¶å: main_train_eval.py (æœ€ç»ˆä¿®æ­£ - è§£å†³æµ‹è¯•é›†é¢„å¤„ç†å¹¶è¡Œé—®é¢˜ + ä¿®å¤æ¨ç†Bug)
# ===================================================================================
from collections import Counter
from sklearn.model_selection import StratifiedKFold
from multiprocessing import Pool, cpu_count
import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset, TensorDataset
from preprocessing import VMD_Sorter_SSA_Processor, stft_transform
from tqdm import tqdm
import numpy as np
import pandas as pd
from model import Simple2DCNN
from use_loss import CombinedLoss, calculate_class_weights
# å¯¼å…¥æ‰€æœ‰ VACWGAN ç»„ä»¶
from vacwgan import (Encoder, Generator, Discriminator, Classifier,
                     reparameterization, compute_gradient_penalty, compute_kl_loss)
from evaluation import evaluate_model  # å¯¼å…¥è¯„ä¼°å‡½æ•°
# å¯¼å…¥æ›´å…¨é¢çš„è¯„ä¼°å·¥å…·
from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,
                             confusion_matrix, classification_report)

import optuna
import shutil
import json
from pathlib import Path
from datetime import datetime
import random

# --- å…¨å±€å¸¸é‡å’Œè®¾ç½® ---
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

TRAIN_ROOT_DIR = r"D:\è½´æ‰¿æ•…éšœæ£€æµ‹ç«èµ›\åˆèµ›è®­ç»ƒé›†"
TEST_ROOT_DIR = r"D:\è½´æ‰¿æ•…éšœæ£€æµ‹ç«èµ›\åˆèµ›æµ‹è¯•é›†"
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# --- ç±»åˆ«æ˜ å°„ (è§£å†³é—®é¢˜ 2) ---
RAW_TO_CLEAN_MAP = {
    'inner_broken_train150': 'inner_broken',
    'inner_wear_train120': 'inner_wear',
    'normal_train160': 'normal',
    'outer_missing_train180': 'outer_missing',
    'roller_broken_train150': 'roller_broken',
    'roller_wear_train100': 'roller_wear'
}
CLEAN_CLASS_NAMES = sorted(list(set(RAW_TO_CLEAN_MAP.values())))
CLEAN_LABEL_MAP = {name: i for i, name in enumerate(CLEAN_CLASS_NAMES)}  # å¹²å‡€æ ‡ç­¾çš„ç´¢å¼•æ˜ å°„

# --- è¶…å‚æ•°è®¾ç½® ---
VACWGAN_EPOCHS = 300
VACWGAN_LR = 1e-4
VACWGAN_SAMPLES_PER_CLASS = 50
LATENT_DIM = 100

N_TRIALS = 300
NUM_EPOCHS = 100
OPTIMIZATION_TIMEOUT = 3600
D_ITERS = 5

# --- å…¨å±€é¢„å¤„ç†å™¨å‚æ•° (ç”¨äºå¤šè¿›ç¨‹ worker) ---
GLOBAL_PREPROCESSOR_PARAMS = {
    'K': 5, 'alpha': 2000, 'window_len': 128, 'ssa_threshold': 32, 'denoise_indices': [0, 1]
}


# --- å¤šè¿›ç¨‹å·¥ä½œå‡½æ•°ï¼ˆè®­ç»ƒé›†ï¼‰ ---
def process_single_signal_worker(path_label_tuple):
    """
    å¤šè¿›ç¨‹æ± ä¸­çš„å·¥ä½œå‡½æ•°ï¼Œå¯¹å•ä¸ªè®­ç»ƒä¿¡å·æ–‡ä»¶è¿›è¡Œé¢„å¤„ç†ã€‚
    è¿”å›: (stft_map, clean_label_int)
    """
    path, raw_label_name = path_label_tuple

    # è½¬æ¢ä¸ºå¹²å‡€æ ‡ç­¾ (åœ¨ worker è¿›ç¨‹ä¸­å®‰å…¨)
    clean_label_name = RAW_TO_CLEAN_MAP.get(raw_label_name, 'unknown')
    label_int = CLEAN_LABEL_MAP.get(clean_label_name, -1)

    preprocessor = VMD_Sorter_SSA_Processor(**GLOBAL_PREPROCESSOR_PARAMS)
    signal = pd.read_excel(path, header=None, usecols=[1]).squeeze("columns").to_numpy()
    denoised_signal = preprocessor.process(signal)
    stft_map = stft_transform(denoised_signal)

    return stft_map, label_int


# --- æ–°å¢ï¼šæµ‹è¯•é›†çš„å¤šè¿›ç¨‹å·¥ä½œå‡½æ•°ï¼ˆé¡¶çº§å‡½æ•°ï¼‰ ---
def process_single_test_signal_worker(path):
    """
    ä¸“é—¨ç”¨äºæµ‹è¯•é›†çš„ worker å‡½æ•°ï¼Œåªæ¥æ”¶è·¯å¾„ï¼Œåªè¿”å›å¤„ç†åçš„ STFT å›¾åƒã€‚
    è§£å†³å¤šè¿›ç¨‹å±€éƒ¨å¯¹è±¡é”™è¯¯ã€‚
    """
    preprocessor = VMD_Sorter_SSA_Processor(**GLOBAL_PREPROCESSOR_PARAMS)
    signal = pd.read_excel(path, header=None, usecols=[1]).squeeze("columns").to_numpy()
    denoised_signal = preprocessor.process(signal)
    stft_map = stft_transform(denoised_signal)
    return stft_map


# --- æ•°æ®é›†ç±» ---
class PreprocessedDataset(Dataset):
    def __init__(self, signals, labels):
        self.signals = signals
        self.labels = labels

    def __len__(self):
        return len(self.signals)

    def __getitem__(self, idx):
        # æ³¨æ„ï¼šè¿™é‡Œæ·»åŠ äº†é€šé“ç»´åº¦ (C=1)
        signal = torch.from_numpy(self.signals[idx]).float().unsqueeze(0)
        label = torch.tensor(self.labels[idx], dtype=torch.long)
        return signal, label


# --- VACWGAN å®Œæ•´è®­ç»ƒå‡½æ•° ---
def train_vacwgan_for_augmentation(data_loader, num_classes, device, epochs):
    """
    å®Œæ•´è®­ç»ƒ VACWGAN ç½‘ç»œ (E, G, D, C)
    """
    print("\n--- å¼€å§‹ VACWGAN å¯¹æŠ—è®­ç»ƒ ---")

    E = Encoder(latent_dim=LATENT_DIM).to(device)
    G = Generator(latent_dim=LATENT_DIM, num_classes=num_classes).to(device)
    D = Discriminator().to(device)
    C = Classifier(num_classes=num_classes).to(device)

    opt_E = optim.Adam(E.parameters(), lr=VACWGAN_LR, betas=(0.5, 0.999))
    opt_G = optim.Adam(G.parameters(), lr=VACWGAN_LR, betas=(0.5, 0.999))
    opt_D = optim.Adam(D.parameters(), lr=VACWGAN_LR, betas=(0.5, 0.999))
    opt_C = optim.Adam(C.parameters(), lr=VACWGAN_LR, betas=(0.5, 0.999))
    ce_loss = nn.CrossEntropyLoss()

    for epoch in range(epochs):
        E.train();
        G.train();
        D.train();
        C.train()

        for i, (real_imgs, labels) in enumerate(data_loader):
            real_imgs, labels = real_imgs.to(device), labels.to(device)

            # --- è®­ç»ƒ Discriminator å’Œ Classifier (D, C) ---
            opt_D.zero_grad();
            opt_C.zero_grad()

            d_real = D(real_imgs);
            c_real = C(real_imgs)
            loss_D_real = -torch.mean(d_real);
            loss_C_real = ce_loss(c_real, labels)

            mu, logvar = E(real_imgs);
            z = reparameterization(mu, logvar)
            fake_imgs = G(z, labels).detach()
            d_fake = D(fake_imgs);
            c_fake = C(fake_imgs)
            loss_D_fake = torch.mean(d_fake);
            loss_C_fake = ce_loss(c_fake, labels)

            gp = compute_gradient_penalty(D, real_imgs.data, fake_imgs.data, device)
            loss_D = loss_D_fake + loss_D_real + gp
            loss_C = loss_C_real + loss_C_fake

            loss_D.backward(retain_graph=True);
            loss_C.backward()
            opt_D.step();
            opt_C.step()

            # --- è®­ç»ƒ Generator å’Œ Encoder (G, E) ---
            if i % D_ITERS == 0:
                opt_G.zero_grad();
                opt_E.zero_grad()

                mu, logvar = E(real_imgs);
                z = reparameterization(mu, logvar)
                fake_imgs = G(z, labels)

                d_fake_g = D(fake_imgs);
                c_fake_g = C(fake_imgs)

                loss_G_adv = -torch.mean(d_fake_g);
                loss_G_cls = ce_loss(c_fake_g, labels)
                loss_E_kl = compute_kl_loss(mu, logvar)

                loss_G_total = loss_G_adv + 10 * loss_G_cls + 1 * loss_E_kl

                loss_G_total.backward()
                opt_G.step();
                opt_E.step()

        if epoch % 10 == 0:
            print(f"Epoch {epoch}/{epochs}, D_Loss: {loss_D.item():.4f}, C_Loss: {loss_C.item():.4f}, G_Loss: {loss_G_total.item():.4f}, KL_Loss: {loss_E_kl.item():.4f}")

    print("--- VACWGAN å¯¹æŠ—è®­ç»ƒç»“æŸ ---")
    return E, G


def load_and_augment_data():
    """åŠ è½½ã€é¢„å¤„ç†ã€è®­ç»ƒ VACWGAN å¹¶è¿›è¡Œæ ·æœ¬å¢å¼º"""
    f_paths, raw_labels = [], []

    # ä»…è·å–åŸå§‹æ–‡ä»¶å¤¹åç§°ï¼Œç”¨äºæ˜ å°„
    raw_class_dirs = sorted([d for d in os.listdir(TRAIN_ROOT_DIR) if os.path.isdir(os.path.join(TRAIN_ROOT_DIR, d))])
    num_classes = len(CLEAN_CLASS_NAMES)

    # 1. åŠ è½½æ‰€æœ‰æ•°æ®è·¯å¾„å’ŒåŸå§‹æ–‡ä»¶å¤¹åç§°
    for raw_name in raw_class_dirs:
        class_dir = os.path.join(TRAIN_ROOT_DIR, raw_name)
        for fname in os.listdir(class_dir):
            if fname.endswith(".xlsx"):
                f_paths.append(os.path.join(class_dir, fname))
                raw_labels.append(raw_name)  # å­˜å‚¨åŸå§‹æ–‡ä»¶å¤¹å

    # 2. é¢„å¤„ç†æ‰€æœ‰æ•°æ® - ä½¿ç”¨ Multiprocessing è¿›è¡Œå¹¶è¡Œè®¡ç®—
    print(f"å¼€å§‹å¯¹ {len(f_paths)} ä¸ªæ ·æœ¬è¿›è¡Œ VMD+SSA+STFT é¢„å¤„ç† (å¹¶è¡Œè®¡ç®—, {cpu_count()} æ ¸å¿ƒ)...")

    # input_tuples åŒ…å« (è·¯å¾„, åŸå§‹æ–‡ä»¶å¤¹åç§°)
    input_tuples = list(zip(f_paths, raw_labels))

    with Pool(cpu_count()) as pool:
        # process_single_signal_worker è¿”å› (stft_map, clean_label_int)
        results = list(tqdm(pool.imap(process_single_signal_worker, input_tuples), total=len(input_tuples), desc="å¹¶è¡Œå¤„ç†ä¿¡å·"))

    X_processed = np.array([res[0] for res in results])
    y_labels = np.array([res[1] for res in results])
    print(f"æ•°æ®é¢„å¤„ç†å®Œæˆã€‚æ ·æœ¬å½¢çŠ¶: {X_processed.shape[1:]}")

    # è¿‡æ»¤æ‰æ ‡ç­¾ä¸º -1 çš„å¼‚å¸¸æ ·æœ¬
    valid_mask = y_labels != -1
    X_processed = X_processed[valid_mask]
    y_labels = y_labels[valid_mask]

    # 3. è®­ç»ƒ VACWGAN
    # æ³¨æ„ï¼šPreprocessedDataset ä¼šè‡ªåŠ¨åœ¨ __getitem__ ä¸­æ·»åŠ é€šé“ç»´åº¦ (N, C, H, W)
    full_dataset = PreprocessedDataset(X_processed, y_labels)
    full_data_loader = DataLoader(full_dataset, batch_size=64, shuffle=True)
    E_trained, G_trained = train_vacwgan_for_augmentation(full_data_loader, num_classes, DEVICE, VACWGAN_EPOCHS)

    # 4. è¯†åˆ«ç¨€ç¼ºç±»å¹¶ç”Ÿæˆæ ·æœ¬
    class_counts = Counter(y_labels)
    majority_count = max(class_counts.values())

    minority_classes_labels = {}
    for raw_name, clean_name in RAW_TO_CLEAN_MAP.items():
        label_int = CLEAN_LABEL_MAP[clean_name]
        # åªè¦æ˜¯ç¨€ç¼ºç±»ï¼Œéƒ½æ·»åŠ åˆ°ç”Ÿæˆç›®æ ‡
        if class_counts.get(label_int, 0) < majority_count:
            minority_classes_labels[label_int] = clean_name

    # 5. æ ·æœ¬åˆæˆå¢å¼º
    X_synthetic_list = [X_processed]
    y_synthetic_list = [y_labels]

    print("\n--- VACWGAN æ ·æœ¬åˆæˆ/å¢å¼ºé˜¶æ®µ ---")

    for label_int in minority_classes_labels.keys():
        X_minority = X_processed[y_labels == label_int]
        y_minority = y_labels[y_labels == label_int]

        num_to_generate = VACWGAN_SAMPLES_PER_CLASS

        def generate_synthetic_samples_from_trained(E_trained, G_trained, X_minority, y_minority, num_to_generate, device):
            """ä½¿ç”¨è®­ç»ƒå¥½çš„ E å’Œ G è¿›è¡Œæ ·æœ¬ç”Ÿæˆ"""
            if X_minority.size == 0 or num_to_generate == 0: return np.array([]), np.array([])
            # ä½¿ç”¨ PreprocessedDataset ä»¥åŒ¹é…è®­ç»ƒæ—¶çš„è¾“å…¥ (N, C, H, W)
            minority_data = PreprocessedDataset(X_minority, y_minority)
            minority_loader = DataLoader(minority_data, batch_size=64, shuffle=True)
            synthetic_X = []
            current_generated = 0
            E_trained.eval();
            G_trained.eval()
            with torch.no_grad():
                while current_generated < num_to_generate:
                    for inputs, labels in minority_loader:
                        inputs, labels = inputs.to(device), labels.to(device)
                        mu, logvar = E_trained(inputs);
                        z = reparameterization(mu, logvar)
                        fake_samples = G_trained(z, labels).cpu().numpy()
                        num_batch = fake_samples.shape[0]
                        if current_generated + num_batch > num_to_generate:
                            needed = num_to_generate - current_generated
                            fake_samples = fake_samples[:needed]
                            num_batch = needed
                        # fake_samples å·²ç»æ˜¯ (N_batch, 1, H, W)ï¼Œæˆ‘ä»¬éœ€è¦ (N_batch, H, W) å­˜å…¥Numpy
                        synthetic_X.append(fake_samples.squeeze(1));
                        current_generated += num_batch
                        if current_generated >= num_to_generate: break
            synthetic_X = np.concatenate(synthetic_X, axis=0)
            synthetic_y = np.full(synthetic_X.shape[0], y_minority[0])
            return synthetic_X, synthetic_y

        X_synth, y_synth = generate_synthetic_samples_from_trained(
            E_trained, G_trained, X_minority, y_minority, num_to_generate, DEVICE
        )

        if X_synth.size > 0:
            X_synthetic_list.append(X_synth)
            y_synthetic_list.append(y_synth)

    X_augmented = np.concatenate(X_synthetic_list, axis=0)
    y_augmented = np.concatenate(y_synthetic_list, axis=0)

    print(f"æ ·æœ¬å¢å¼ºå®Œæ¯•ã€‚åŸå§‹æ€»æ•°: {len(y_labels)}, å¢å¼ºåæ€»æ•°: {len(y_augmented)}")

    return X_augmented, y_augmented, CLEAN_CLASS_NAMES


# --- Optuna ä¼˜åŒ–ç›®æ ‡å‡½æ•° (å·²ä¿®å¤è­¦å‘Š + ä¿®å¤Bug) ---
def objective_cv(trial, X_all, y_all, num_classes, device, optimization_dir, n_folds=5):
    """Optunaä¼˜åŒ–ç›®æ ‡å‡½æ•°ï¼ˆ5æŠ˜äº¤å‰éªŒè¯ï¼‰"""

    # --- è¶…å‚æ•°æœç´¢ç©ºé—´ (ç•¥) ---
    learning_rate = trial.suggest_float('lr', 1e-5, 1e-3, log=True)
    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64])
    dropout_rate = trial.suggest_float('dropout', 0.1, 0.6)
    weight_decay = trial.suggest_float('weight_decay', 1e-6, 1e-3, log=True)
    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'AdamW'])
    max_grad_norm = trial.suggest_float('max_grad_norm', 0.5, 2.0)
    use_focal_loss = trial.suggest_categorical('use_focal', [True, False])
    use_label_smoothing = trial.suggest_categorical('use_ls', [True, False])
    use_center_loss = trial.suggest_categorical('use_center', [True, False])
    focal_alpha = trial.suggest_float('focal_alpha', 0.1, 0.5) if use_focal_loss else 0.25
    focal_gamma = trial.suggest_float('focal_gamma', 1.0, 3.0) if use_focal_loss else 2.0
    label_smoothing_factor = trial.suggest_float('ls_factor', 0.05, 0.2) if use_label_smoothing else 0.1
    center_loss_weight = trial.suggest_float('center_w', 0.001, 0.01, log=True) if use_center_loss else 0.003

    try:
        skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=SEED)
        fold_accuracies = []
        global_step = 0

        best_trial_fold_state = None
        best_trial_fold_acc = -1.0

        # ç”¨äºè®°å½•æœ€ä½³ fold çš„æ ‡ç­¾å’Œé¢„æµ‹ç»“æœ (ç”¨äºè¯¦ç»†è¯„ä¼°)
        best_val_labels = []
        best_val_preds = []

        # --- (ä¿®å¤ Bug) æ–°å¢ï¼šç”¨äºå­˜å‚¨æœ€ä½³æŠ˜çš„ç»Ÿè®¡æ•°æ® ---
        best_fold_mean = None
        best_fold_std = None

        for fold_idx, (train_idx, val_idx) in enumerate(skf.split(X_all, y_all)):
            X_train, y_train = X_all[train_idx], y_all[train_idx]
            X_val, y_val = X_all[val_idx], y_all[val_idx]

            # --- (ä¿®å¤ Bug) å…³é”®ï¼šæ•è·å½“å‰æŠ˜çš„ç»Ÿè®¡æ•°æ® ---
            current_fold_mean = X_train.mean(axis=(0, 1, 2), keepdims=True)
            current_fold_std = X_train.std(axis=(0, 1, 2), keepdims=True) + 1e-8

            # åº”ç”¨å½’ä¸€åŒ–
            X_train_norm = (X_train - current_fold_mean) / current_fold_std
            X_val_norm = (X_val - current_fold_mean) / current_fold_std

            model = Simple2DCNN(num_classes=num_classes, dropout=dropout_rate).to(device)
            center_feature_dim = model.center_feature_dim

            # --- æ³¨æ„ï¼šä½¿ç”¨ TensorDatasetï¼Œå®ƒä¸ä¼šè‡ªåŠ¨æ·»åŠ é€šé“ç»´åº¦ ---
            train_dataset = TensorDataset(torch.FloatTensor(X_train_norm), torch.LongTensor(y_train))
            val_dataset = TensorDataset(torch.FloatTensor(X_val_norm), torch.LongTensor(y_val))
            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
            val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
            class_weights = calculate_class_weights(y_train)

            criterion = CombinedLoss(
                num_classes=num_classes, feature_dim=center_feature_dim, use_focal=use_focal_loss,
                use_label_smoothing=use_label_smoothing, use_center_loss=use_center_loss,
                focal_alpha=focal_alpha, focal_gamma=focal_gamma, label_smoothing_factor=label_smoothing_factor,
                center_loss_weight=center_loss_weight, class_weights=class_weights.to(device)
            )

            if optimizer_name == 'Adam':
                optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)
            else:
                optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)

            scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10)

            fold_best_acc = 0

            # --- (ä¿®å¤ Bug) æ–°å¢ï¼šç”¨äºå­˜å‚¨å½“å‰æŠ˜çš„æœ€ä½³çŠ¶æ€ ---
            current_fold_best_state = None
            current_fold_best_labels = []
            current_fold_best_preds = []

            for epoch in range(NUM_EPOCHS):
                model.train()
                for X_batch, y_batch in train_loader:
                    # --- (ä¿®å¤ Bug) æ‰‹åŠ¨æ·»åŠ é€šé“ç»´åº¦ (C=1)ï¼Œå› ä¸ºç”¨çš„æ˜¯ TensorDataset ---
                    X_batch = X_batch.to(device).unsqueeze(1);
                    y_batch = y_batch.to(device)
                    optimizer.zero_grad()
                    outputs, features = model(X_batch, return_features=True)
                    loss, _ = criterion(outputs, features, y_batch)
                    loss.backward()
                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=max_grad_norm)
                    optimizer.step()

                model.eval()
                val_loss = 0
                val_correct = 0
                val_total = 0
                current_val_labels = [];
                current_val_preds = []

                with torch.no_grad():
                    for X_batch, y_batch in val_loader:
                        # --- (ä¿®å¤ Bug) æ‰‹åŠ¨æ·»åŠ é€šé“ç»´åº¦ (C=1) ---
                        X_batch = X_batch.to(device).unsqueeze(1);
                        y_batch = y_batch.to(device)
                        outputs, features = model(X_batch, return_features=True)
                        val_loss += criterion(outputs, features, y_batch)[0].item() * X_batch.size(0)
                        _, predicted = torch.max(outputs.data, 1)
                        val_total += y_batch.size(0)
                        val_correct += (predicted == y_batch).sum().item()
                        current_val_labels.extend(y_batch.cpu().numpy())
                        current_val_preds.extend(predicted.cpu().numpy())

                val_acc = 100 * val_correct / val_total
                val_loss /= val_total
                scheduler.step(val_loss)

                trial.report(val_acc, global_step)

                # --- (ä¿®å¤ Bug) é€»è¾‘ä¿®æ­£ï¼šä¿å­˜å½“å‰æŠ˜çš„æœ€ä½³æ¨¡å‹çŠ¶æ€ ---
                if val_acc > fold_best_acc:
                    fold_best_acc = val_acc
                    current_fold_best_state = model.state_dict().copy()
                    current_fold_best_labels = current_val_labels
                    current_fold_best_preds = current_val_preds

                if trial.should_prune():
                    raise optuna.exceptions.TrialPruned()

                global_step += 1

            fold_accuracies.append(fold_best_acc)

            # --- (ä¿®å¤ Bug) é€»è¾‘ä¿®æ­£ï¼šæ¯”è¾ƒå½“å‰æŠ˜çš„æœ€ä½³accä¸è¯•éªŒçš„å…¨å±€æœ€ä½³acc ---
            if fold_best_acc > best_trial_fold_acc:
                best_trial_fold_acc = fold_best_acc
                best_trial_fold_state = current_fold_best_state  # ä¿å­˜æœ€ä½³æŠ˜çš„æœ€ä½³çŠ¶æ€
                best_val_labels = current_fold_best_labels  # ä¿å­˜å¯¹åº”çš„æ ‡ç­¾
                best_val_preds = current_fold_best_preds  # ä¿å­˜å¯¹åº”çš„é¢„æµ‹

                # --- (ä¿®å¤ Bug) ä¿å­˜ä¸æœ€ä½³æ¨¡å‹åŒ¹é…çš„å½’ä¸€åŒ–ç»Ÿè®¡æ•°æ® ---
                best_fold_mean = current_fold_mean
                best_fold_std = current_fold_std

        mean_acc = np.mean(fold_accuracies)

        if best_trial_fold_state is not None:
            trial.set_user_attr("best_model_state", best_trial_fold_state)
            trial.set_user_attr("best_val_labels", best_val_labels)
            trial.set_user_attr("best_val_preds", best_val_preds)

            # --- (ä¿®å¤ Bug) å°†ç»Ÿè®¡æ•°æ®å­˜å…¥è¯•éªŒå±æ€§ ---
            if best_fold_mean is not None and best_fold_std is not None:
                trial.set_user_attr("best_model_mean", best_fold_mean.tolist())
                trial.set_user_attr("best_model_std", best_fold_std.tolist())

        return mean_acc

    except Exception as e:
        print(f"Trial {trial.number} failed with error: {e}")
        return 0.0


# --- (æ»¡è¶³è¯·æ±‚ 1) é‡å†™è¯¦ç»†æŒ‡æ ‡å‡½æ•° ---
def output_detailed_metrics(y_true, y_pred, class_names, output_filepath=None):
    """
    è¾“å‡ºå¹¶å¯¼å‡ºè¯¦ç»†çš„åˆ†ç±»æŒ‡æ ‡å’Œæ··æ·†çŸ©é˜µ (è§£å†³é—®é¢˜ 3)
    """
    report_lines = []

    report_lines.append("--- éªŒè¯é›†è¯¦ç»†æ€§èƒ½æŠ¥å‘Š ---")

    # å®å¹³å‡æŒ‡æ ‡
    accuracy = accuracy_score(y_true, y_pred)
    macro_f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)
    macro_precision = precision_score(y_true, y_pred, average='macro', zero_division=0)
    macro_recall = recall_score(y_true, y_pred, average='macro', zero_division=0)

    report_lines.append("\n--- æ•´ä½“å®å¹³å‡æŒ‡æ ‡ ---")
    report_lines.append(f"æ€»ä½“å‡†ç¡®ç‡ (Accuracy): {accuracy:.3f}")
    report_lines.append(f"å®å¹³å‡F1åˆ†æ•° (Macro-F1): {macro_f1:.3f}")
    report_lines.append(f"å®å¹³å‡ç²¾ç¡®ç‡ (Macro-Precision): {macro_precision:.3f}")
    report_lines.append(f"å®å¹³å‡å¬å›ç‡ (Macro-Recall): {macro_recall:.3f}")

    # åˆ†ç±»æŠ¥å‘Š (åŒ…å«æ¯ä¸ªç±»åˆ«çš„ P, R, F1)
    report_lines.append("\n--- å„ç±»åˆ«è¯¦ç»†æŒ‡æ ‡ (Precision, Recall, F1-Score) ---")
    # ä½¿ç”¨ classification_report è‡ªåŠ¨ç”Ÿæˆ P, R, F1
    class_report = classification_report(
        y_true, y_pred, target_names=class_names, digits=3, zero_division=0
    )
    report_lines.append(class_report)

    # æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(y_true, y_pred)
    report_lines.append("\n--- æ··æ·†çŸ©é˜µ ---")
    report_lines.append(f"ç±»åˆ«ç´¢å¼•: {CLEAN_LABEL_MAP}")
    report_lines.append("çŸ©é˜µ (è¡Œ: çœŸå®æ ‡ç­¾, åˆ—: é¢„æµ‹æ ‡ç­¾):")
    report_lines.append(str(cm))

    # --- æ‰“å°åˆ°æ§åˆ¶å° ---
    print("\n".join(report_lines))

    # --- (æ»¡è¶³è¯·æ±‚ 1) å¯¼å‡ºåˆ°æ–‡ä»¶ ---
    if output_filepath:
        try:
            with open(output_filepath, 'w', encoding='utf-8') as f:
                f.write("\n".join(report_lines))
            print(f"\nâœ… è¯¦ç»†æ€§èƒ½æŠ¥å‘Šå·²å¯¼å‡ºè‡³: {output_filepath}")
        except Exception as e:
            print(f"\nâš ï¸ å¯¼å‡ºæ€§èƒ½æŠ¥å‘Šå¤±è´¥: {e}")


def hyperparameter_optimization():
    """ä¸»å‡½æ•°ï¼Œè´Ÿè´£é©±åŠ¨ VACWGAN è®­ç»ƒã€å¢å¼ºå’Œ Optuna è°ƒä¼˜"""
    print("\n" + "=" * 60)
    print("ğŸ” å¼€å§‹è¶…å‚æ•°ä¼˜åŒ–ï¼ˆ5æŠ˜äº¤å‰éªŒè¯ï¼‰")
    print("=" * 60)

    X_all, y_all, class_names = load_and_augment_data()
    num_classes = len(class_names)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    optimization_dir = f"./hyperparameter_optimization_{timestamp}"
    Path(optimization_dir).mkdir(parents=True, exist_ok=True)

    study = optuna.create_study(
        direction='maximize',
        sampler=optuna.samplers.TPESampler(seed=SEED),
        pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=10)
    )

    def objective_wrapper(trial):
        return objective_cv(
            trial, X_all, y_all, num_classes, DEVICE, optimization_dir, n_folds=5
        )

    print(f"\nå¼€å§‹ä¼˜åŒ– (å…±{N_TRIALS}æ¬¡è¯•éªŒ)...")
    study.optimize(
        objective_wrapper,
        n_trials=N_TRIALS,
        timeout=OPTIMIZATION_TIMEOUT,
        show_progress_bar=True
    )

    best_trial = study.best_trial
    best_params = best_trial.params

    # 4. ä¿å­˜æœ€ä½³æ¨¡å‹
    if "best_model_state" in best_trial.user_attrs:
        best_model_state = best_trial.user_attrs["best_model_state"]
        best_model_path = os.path.join(optimization_dir, "best_model.pth")
        torch.save(best_model_state, best_model_path)
        print(f"\nâœ… æœ€ä½³æ¨¡å‹æƒé‡å·²ä¿å­˜è‡³: {best_model_path}")

        # --- (ä¿®å¤ Bug) æ–°å¢ï¼šä¿å­˜å½’ä¸€åŒ–ç»Ÿè®¡æ•°æ® ---
        if "best_model_mean" in best_trial.user_attrs and "best_model_std" in best_trial.user_attrs:
            best_mean = np.array(best_trial.user_attrs["best_model_mean"])
            best_std = np.array(best_trial.user_attrs["best_model_std"])
            np.save(os.path.join(optimization_dir, "best_model_mean.npy"), best_mean)
            np.save(os.path.join(optimization_dir, "best_model_std.npy"), best_std)
            print(f"âœ… æœ€ä½³æ¨¡å‹å¯¹åº”çš„å½’ä¸€åŒ–ç»Ÿè®¡æ•°æ®å·²ä¿å­˜ã€‚")
        else:
            print(f"âš ï¸ è­¦å‘Š: æœªæ‰¾åˆ°æœ€ä½³æ¨¡å‹çš„å½’ä¸€åŒ–ç»Ÿè®¡æ•°æ®ã€‚æµ‹è¯•é›†æ¨ç†å¯èƒ½å¤±è´¥ã€‚")

        # --- (æ»¡è¶³è¯·æ±‚ 1) ä¿®æ­£ï¼šè¾“å‡ºå¹¶å¯¼å‡ºéªŒè¯é›†è¯¦ç»†æ€§èƒ½æŠ¥å‘Š ---
        if "best_val_labels" in best_trial.user_attrs:
            y_true_val = np.array(best_trial.user_attrs["best_val_labels"])
            y_pred_val = np.array(best_trial.user_attrs["best_val_preds"])

            # å®šä¹‰æŠ¥å‘Šè·¯å¾„
            val_report_path = os.path.join(optimization_dir, "validation_performance_report.txt")
            # è°ƒç”¨æ–°å‡½æ•°
            output_detailed_metrics(y_true_val, y_pred_val, class_names, val_report_path)
    else:
        print(f"\nâš ï¸ æœªæ‰¾åˆ°æœ€ä½³æ¨¡å‹çŠ¶æ€å­—å…¸ã€‚è¯·æ£€æŸ¥ Optuna è¿è¡Œæƒ…å†µã€‚")

    # 5. ä¿å­˜æœ€ä½³è¶…å‚æ•°
    best_params_path = os.path.join(optimization_dir, "best_params.json")
    with open(best_params_path, 'w') as f:
        json.dump(best_params, f, indent=4)
    print(f"âœ… æœ€ä½³è¶…å‚æ•°å·²ä¿å­˜è‡³: {best_params_path}")

    print(f"\nâœ… ä¼˜åŒ–å®Œæˆ! æœ€ä½³å¹³å‡éªŒè¯å‡†ç¡®ç‡: {study.best_value:.3f}%")

    return best_params, optimization_dir, class_names


# --- (ä¿®å¤ Bug) ä¿®æ­£æµ‹è¯•é›†æ¨ç†å‡½æ•° ---
def run_test_inference(optimization_dir, class_names, best_params):
    """åŠ è½½æœ€ä½³æ¨¡å‹ï¼Œå¤„ç†æœ€ç»ˆæµ‹è¯•é›†ï¼Œå¹¶è¾“å‡ºè¦æ±‚çš„ TXT æ ¼å¼ç»“æœ (å·²ä¿®å¤å¹¶è¡Œé¢„å¤„ç†å’Œæ ‡ç­¾é—®é¢˜)"""

    # 1. åŠ è½½æœ€ä½³æ¨¡å‹
    num_classes = len(class_names)
    dropout_rate = best_params.get('dropout', 0.5)
    model = Simple2DCNN(num_classes=num_classes, dropout=dropout_rate).to(DEVICE)
    best_model_path = os.path.join(optimization_dir, "best_model.pth")

    try:
        model.load_state_dict(torch.load(best_model_path, map_location=DEVICE))
        print(f"\næˆåŠŸåŠ è½½æœ€ä½³æ¨¡å‹: {best_model_path}")
    except FileNotFoundError:
        print(f"\né”™è¯¯: æœªæ‰¾åˆ°æœ€ä½³æ¨¡å‹æƒé‡æ–‡ä»¶ã€‚è¯·æ£€æŸ¥è·¯å¾„ã€‚")
        return

    # --- (ä¿®å¤ Bug) æ–°å¢ï¼šåŠ è½½å½’ä¸€åŒ–ç»Ÿè®¡æ•°æ® ---
    mean_path = os.path.join(optimization_dir, "best_model_mean.npy")
    std_path = os.path.join(optimization_dir, "best_model_std.npy")
    try:
        data_mean = np.load(mean_path)
        data_std = np.load(std_path)
        print("æˆåŠŸåŠ è½½å½’ä¸€åŒ–ç»Ÿè®¡æ•°æ® (mean/std)ã€‚")
    except FileNotFoundError:
        print(f"\né”™è¯¯: æœªæ‰¾åˆ°å½’ä¸€åŒ–ç»Ÿè®¡æ–‡ä»¶ (mean/std)ã€‚")
        print(f"è¯·ç¡®ä¿ 'best_model_mean.npy' å’Œ 'best_model_std.npy' ä½äº {optimization_dir} ç›®å½•ä¸‹ã€‚")
        return

    # 2. é¢„å¤„ç†æµ‹è¯•é›†æ•°æ® (ä½¿ç”¨ Multiprocessing è¿›è¡Œå¹¶è¡Œè®¡ç®—)
    test_files = [f for f in os.listdir(TEST_ROOT_DIR) if f.endswith(".xlsx")]
    final_test_paths = [os.path.join(TEST_ROOT_DIR, f) for f in test_files]

    # æ„å»ºå¤šè¿›ç¨‹è¾“å…¥åˆ—è¡¨ï¼šåªåŒ…å«è·¯å¾„ (ç”¨äº process_single_test_signal_worker)
    inference_input_paths = final_test_paths

    print(f"\nå¼€å§‹å¤„ç†æœ€ç»ˆæµ‹è¯•é›†æ•°æ® (å¹¶è¡Œè®¡ç®—, {cpu_count()} æ ¸å¿ƒ)...")

    # è°ƒç”¨é¡¶çº§çš„ process_single_test_signal_worker
    with Pool(cpu_count()) as pool:
        X_final_test_processed = list(tqdm(pool.imap(process_single_test_signal_worker, inference_input_paths), total=len(inference_input_paths), desc="å¹¶è¡Œå¤„ç†æµ‹è¯•ä¿¡å·"))

    X_final_test_processed = np.array(X_final_test_processed)

    # --- (ä¿®å¤ Bug) æ–°å¢ï¼šå¯¹æµ‹è¯•é›†åº”ç”¨å½’ä¸€åŒ– ---
    print("åº”ç”¨å½’ä¸€åŒ–åˆ°æµ‹è¯•é›†...")
    X_final_test_norm = (X_final_test_processed - data_mean) / data_std

    # 3. è¿è¡Œæ¨ç†
    # --- (ä¿®å¤ Bug) ä½¿ç”¨å½’ä¸€åŒ–åçš„æ•°æ® (X_final_test_norm) ---
    # PreprocessedDataset ä¼šè‡ªåŠ¨æ·»åŠ é€šé“ç»´åº¦
    final_test_dataset = PreprocessedDataset(X_final_test_norm, [0] * len(X_final_test_norm))
    final_test_loader = DataLoader(final_test_dataset, batch_size=64, shuffle=False)

    model.eval()
    all_preds = []
    with torch.no_grad():
        for inputs, _ in final_test_loader:
            # inputs å·²ç»å…·æœ‰ (N, 1, H, W) å½¢çŠ¶ï¼Œç”± PreprocessedDataset æä¾›
            inputs = inputs.to(DEVICE)
            outputs = model(inputs)
            _, predicted = torch.max(outputs.data, 1)
            all_preds.extend(predicted.cpu().numpy())

    y_pred_final = np.array(all_preds)

    # 4. å¯¼å‡ºé¢„æµ‹ç»“æœ (ä½¿ç”¨ CLEAN_CLASS_NAMES)
    output_filename = "test_prediction_results.txt"
    with open(output_filename, 'w', encoding='utf-8') as f:
        f.write("æµ‹è¯•é›†åç§°\tæ•…éšœç±»å‹\n")

        for path, pred_index in zip(final_test_paths, y_pred_final):
            base_name = os.path.basename(path)
            test_set_name = os.path.splitext(base_name)[0]

            # ä¿®æ­£ï¼šä½¿ç”¨ CLEAN_CLASS_NAMES åˆ—è¡¨è¿›è¡Œç´¢å¼•æ˜ å°„
            predicted_fault_type = class_names[pred_index]

            f.write(f"{test_set_name}\t{predicted_fault_type}\n")

    print(f"\nâœ… æµ‹è¯•é›†é¢„æµ‹ç»“æœå·²å¯¼å‡ºè‡³: '{output_filename}'")

    # é¢å¤–æ£€æŸ¥ï¼šç¡®è®¤é¢„æµ‹ç»“æœæ˜¯å¦å¤šæ ·åŒ–
    pred_counts = Counter(y_pred_final)
    print(f"\n--- æµ‹è¯•é›†é¢„æµ‹åˆ†å¸ƒæ¦‚è§ˆ (å…± {len(y_pred_final)} ä¸ªæ ·æœ¬) ---")
    for pred_idx, count in pred_counts.items():
        print(f"  {class_names[pred_idx]:<20}: {count} ä¸ª")


if __name__ == '__main__':
    # --- é˜¶æ®µ 1: æ ·æœ¬å¢å¼ºä¸è¶…å‚æ•°ä¼˜åŒ– ---
    best_params, optimization_dir, class_names = hyperparameter_optimization()

    # --- é˜¶æ®µ 2: æœ€ç»ˆæµ‹è¯•é›†è¯„ä¼° ---
    print("\n" + "=" * 60)
    print("ğŸš€ è¿è¡Œæœ€ç»ˆæµ‹è¯•é›†æ¨ç†")
    print("=" * 60)
    run_test_inference(optimization_dir, class_names, best_params)