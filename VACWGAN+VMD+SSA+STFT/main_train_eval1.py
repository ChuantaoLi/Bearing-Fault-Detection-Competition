# ===================================================================================
# æ–‡ä»¶å: main_train_eval.py (æœ€ç»ˆä¿®æ­£ - é›†æˆå­¦ä¹  + æ­£ç¡®çš„CVå¢å¼º)
# æè¿°:
# 1. [ä¿®å¤æ•°æ®æ³„éœ²] åœ¨ K-Fold å¾ªç¯å†…éƒ¨è¿›è¡Œæ•°æ®å¢å¼ºï¼Œåœ¨çº¯å‡€éªŒè¯é›†ä¸Šè¯„ä¼°ã€‚
# 2. [é›†æˆå­¦ä¹ ] Optuna ä»…ç”¨äºå¯»æ‰¾æœ€ä½³è¶…å‚æ•°ã€‚
# 3. [é›†æˆå­¦ä¹ ] æ‰¾åˆ°æœ€ä½³å‚æ•°åï¼Œé‡æ–°è®­ç»ƒ 5 ä¸ª K-Fold æ¨¡å‹ç”¨äºé›†æˆã€‚
# 4. [é›†æˆå­¦ä¹ ] æ¨ç†æ—¶åŠ è½½ 5 ä¸ªæ¨¡å‹ï¼Œä½¿ç”¨å¹³å‡ Logits è¿›è¡Œé¢„æµ‹ã€‚
# ===================================================================================
from collections import Counter
from sklearn.model_selection import StratifiedKFold
from multiprocessing import Pool, cpu_count
import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset, TensorDataset
from preprocessing import VMD_Sorter_SSA_Processor, stft_transform
from tqdm import tqdm
import numpy as np
import pandas as pd
from model import Simple2DCNN
from use_loss import CombinedLoss, calculate_class_weights
# å¯¼å…¥æ‰€æœ‰ VACWGAN ç»„ä»¶
from vacwgan import (Encoder, Generator, Discriminator, Classifier,
                     reparameterization, compute_gradient_penalty, compute_kl_loss)
# å¯¼å…¥æ›´å…¨é¢çš„è¯„ä¼°å·¥å…·
from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,
                             confusion_matrix, classification_report)

import optuna
import shutil
import json
from pathlib import Path
from datetime import datetime
import random

# --- å…¨å±€å¸¸é‡å’Œè®¾ç½® ---
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

TRAIN_ROOT_DIR = r"D:\è½´æ‰¿æ•…éšœæ£€æµ‹ç«èµ›\åˆèµ›è®­ç»ƒé›†"
TEST_ROOT_DIR = r"D:\è½´æ‰¿æ•…éšœæ£€æµ‹ç«èµ›\åˆèµ›æµ‹è¯•é›†"
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# --- ç±»åˆ«æ˜ å°„ ---
RAW_TO_CLEAN_MAP = {
    'inner_broken_train150': 'inner_broken',
    'inner_wear_train120': 'inner_wear',
    'normal_train160': 'normal',
    'outer_missing_train180': 'outer_missing',
    'roller_broken_train150': 'roller_broken',
    'roller_wear_train100': 'roller_wear'
}
CLEAN_CLASS_NAMES = sorted(list(set(RAW_TO_CLEAN_MAP.values())))
CLEAN_LABEL_MAP = {name: i for i, name in enumerate(CLEAN_CLASS_NAMES)}

# --- è¶…å‚æ•°è®¾ç½® ---
VACWGAN_EPOCHS = 300
VACWGAN_LR = 1e-4
VACWGAN_SAMPLES_PER_CLASS = 50
LATENT_DIM = 100
N_FOLDS = 5  # äº¤å‰éªŒè¯æŠ˜æ•°

N_TRIALS = 300  # (å‡å°‘è¯•éªŒæ¬¡æ•°ä»¥ä¾¿æ¼”ç¤ºï¼Œä½ å¯ä»¥è°ƒå› 300)
NUM_EPOCHS = 100
OPTIMIZATION_TIMEOUT = 10800
D_ITERS = 5

# --- å…¨å±€é¢„å¤„ç†å™¨å‚æ•° ---
GLOBAL_PREPROCESSOR_PARAMS = {
    'K': 5, 'alpha': 2000, 'window_len': 128, 'ssa_threshold': 32, 'denoise_indices': [0, 1]
}


# --- å¤šè¿›ç¨‹å·¥ä½œå‡½æ•°ï¼ˆè®­ç»ƒé›†ï¼‰ ---
def process_single_signal_worker(path_label_tuple):
    path, raw_label_name = path_label_tuple
    clean_label_name = RAW_TO_CLEAN_MAP.get(raw_label_name, 'unknown')
    label_int = CLEAN_LABEL_MAP.get(clean_label_name, -1)
    preprocessor = VMD_Sorter_SSA_Processor(**GLOBAL_PREPROCESSOR_PARAMS)
    signal = pd.read_excel(path, header=None, usecols=[1]).squeeze("columns").to_numpy()
    denoised_signal = preprocessor.process(signal)
    stft_map = stft_transform(denoised_signal)
    return stft_map, label_int


# --- æµ‹è¯•é›†çš„å¤šè¿›ç¨‹å·¥ä½œå‡½æ•° ---
def process_single_test_signal_worker(path):
    preprocessor = VMD_Sorter_SSA_Processor(**GLOBAL_PREPROCESSOR_PARAMS)
    signal = pd.read_excel(path, header=None, usecols=[1]).squeeze("columns").to_numpy()
    denoised_signal = preprocessor.process(signal)
    stft_map = stft_transform(denoised_signal)
    return stft_map


# --- æ•°æ®é›†ç±» ---
class PreprocessedDataset(Dataset):
    def __init__(self, signals, labels):
        self.signals = signals
        self.labels = labels

    def __len__(self):
        return len(self.signals)

    def __getitem__(self, idx):
        # æ³¨æ„ï¼šè¿™é‡Œæ·»åŠ äº†é€šé“ç»´åº¦ (C=1)
        signal = torch.from_numpy(self.signals[idx]).float().unsqueeze(0)
        label = torch.tensor(self.labels[idx], dtype=torch.long)
        return signal, label


# --- VACWGAN å®Œæ•´è®­ç»ƒå‡½æ•° ---
def train_vacwgan_for_augmentation(data_loader, num_classes, device, epochs):
    print("\n--- å¼€å§‹ VACWGAN å¯¹æŠ—è®­ç»ƒ ---")
    E = Encoder(latent_dim=LATENT_DIM).to(device)
    G = Generator(latent_dim=LATENT_DIM, num_classes=num_classes).to(device)
    D = Discriminator().to(device)
    C = Classifier(num_classes=num_classes).to(device)
    opt_E = optim.Adam(E.parameters(), lr=VACWGAN_LR, betas=(0.5, 0.999))
    opt_G = optim.Adam(G.parameters(), lr=VACWGAN_LR, betas=(0.5, 0.999))
    opt_D = optim.Adam(D.parameters(), lr=VACWGAN_LR, betas=(0.5, 0.999))
    opt_C = optim.Adam(C.parameters(), lr=VACWGAN_LR, betas=(0.5, 0.999))
    ce_loss = nn.CrossEntropyLoss()

    for epoch in range(epochs):
        E.train();
        G.train();
        D.train();
        C.train()
        for i, (real_imgs, labels) in enumerate(data_loader):
            real_imgs, labels = real_imgs.to(device), labels.to(device)
            # --- è®­ç»ƒ D, C ---
            opt_D.zero_grad();
            opt_C.zero_grad()
            d_real = D(real_imgs);
            c_real = C(real_imgs)
            loss_D_real = -torch.mean(d_real);
            loss_C_real = ce_loss(c_real, labels)
            mu, logvar = E(real_imgs);
            z = reparameterization(mu, logvar)
            fake_imgs = G(z, labels).detach()
            d_fake = D(fake_imgs);
            c_fake = C(fake_imgs)
            loss_D_fake = torch.mean(d_fake);
            loss_C_fake = ce_loss(c_fake, labels)
            gp = compute_gradient_penalty(D, real_imgs.data, fake_imgs.data, device)
            loss_D = loss_D_fake + loss_D_real + gp
            loss_C = loss_C_real + loss_C_fake
            loss_D.backward(retain_graph=True);
            loss_C.backward()
            opt_D.step();
            opt_C.step()
            # --- è®­ç»ƒ G, E ---
            if i % D_ITERS == 0:
                opt_G.zero_grad();
                opt_E.zero_grad()
                mu, logvar = E(real_imgs);
                z = reparameterization(mu, logvar)
                fake_imgs = G(z, labels)
                d_fake_g = D(fake_imgs);
                c_fake_g = C(fake_imgs)
                loss_G_adv = -torch.mean(d_fake_g);
                loss_G_cls = ce_loss(c_fake_g, labels)
                loss_E_kl = compute_kl_loss(mu, logvar)
                loss_G_total = loss_G_adv + 10 * loss_G_cls + 1 * loss_E_kl
                loss_G_total.backward()
                opt_G.step();
                opt_E.step()
        if epoch % 20 == 0:
            print(
                f"Epoch {epoch}/{epochs}, D_Loss: {loss_D.item():.4f}, C_Loss: {loss_C.item():.4f}, G_Loss: {loss_G_total.item():.4f}")
    print("--- VACWGAN å¯¹æŠ—è®­ç»ƒç»“æŸ ---")
    return E, G


# --- [NEW] æ ·æœ¬ç”Ÿæˆå‡½æ•°ï¼ˆä» load_and_augment_data ç§»å‡ºï¼‰ ---
def generate_synthetic_samples_from_trained(E_trained, G_trained, X_minority, y_minority, num_to_generate, device):
    """ä½¿ç”¨è®­ç»ƒå¥½çš„ E å’Œ G è¿›è¡Œæ ·æœ¬ç”Ÿæˆ"""
    if X_minority.size == 0 or num_to_generate == 0: return np.array([]), np.array([])
    # ä½¿ç”¨ PreprocessedDataset ä»¥åŒ¹é…è®­ç»ƒæ—¶çš„è¾“å…¥ (N, C, H, W)
    minority_data = PreprocessedDataset(X_minority, y_minority)
    # ç¡®ä¿ batch_size ä¸å¤§äºæ ·æœ¬æ•°
    batch_size = min(64, len(minority_data))
    if batch_size == 0: return np.array([]), np.array([])
    minority_loader = DataLoader(minority_data, batch_size=batch_size, shuffle=True)

    synthetic_X = []
    current_generated = 0
    E_trained.eval();
    G_trained.eval()
    with torch.no_grad():
        while current_generated < num_to_generate:
            for inputs, labels in minority_loader:
                inputs, labels = inputs.to(device), labels.to(device)
                mu, logvar = E_trained(inputs);
                z = reparameterization(mu, logvar)
                fake_samples = G_trained(z, labels).cpu().numpy()
                num_batch = fake_samples.shape[0]
                if current_generated + num_batch > num_to_generate:
                    needed = num_to_generate - current_generated
                    fake_samples = fake_samples[:needed]
                    num_batch = needed
                synthetic_X.append(fake_samples.squeeze(1));  # (N, 1, H, W) -> (N, H, W)
                current_generated += num_batch
                if current_generated >= num_to_generate: break
            # å¦‚æœæ ·æœ¬é‡å¤ªå°‘ï¼Œloader å¾ªç¯ä¸€æ¬¡å°±ç»“æŸäº†ï¼Œéœ€è¦æ‰‹åŠ¨ break
            if current_generated >= num_to_generate: break
            # é˜²æ­¢å›  batch_size=0 å¯¼è‡´çš„æ­»å¾ªç¯
            if batch_size == 0: break

    if not synthetic_X: return np.array([]), np.array([])
    synthetic_X = np.concatenate(synthetic_X, axis=0)
    synthetic_y = np.full(synthetic_X.shape[0], y_minority[0])
    return synthetic_X, synthetic_y


# --- [MODIFIED] é‡å‘½åï¼šåªåŠ è½½çº¯å‡€æ•°æ®å¹¶è®­ç»ƒ GAN ---
def load_pure_data_and_train_gan():
    """åŠ è½½ã€é¢„å¤„ç†çº¯å‡€æ•°æ®ï¼Œå¹¶è®­ç»ƒ VACWGAN"""
    f_paths, raw_labels = [], []
    raw_class_dirs = sorted([d for d in os.listdir(TRAIN_ROOT_DIR) if os.path.isdir(os.path.join(TRAIN_ROOT_DIR, d))])
    num_classes = len(CLEAN_CLASS_NAMES)

    for raw_name in raw_class_dirs:
        class_dir = os.path.join(TRAIN_ROOT_DIR, raw_name)
        for fname in os.listdir(class_dir):
            if fname.endswith(".xlsx"):
                f_paths.append(os.path.join(class_dir, fname))
                raw_labels.append(raw_name)

    print(f"å¼€å§‹å¯¹ {len(f_paths)} ä¸ª[çº¯å‡€]æ ·æœ¬è¿›è¡Œ VMD+SSA+STFT é¢„å¤„ç† (å¹¶è¡Œ, {cpu_count()} æ ¸å¿ƒ)...")
    input_tuples = list(zip(f_paths, raw_labels))
    with Pool(cpu_count()) as pool:
        results = list(
            tqdm(pool.imap(process_single_signal_worker, input_tuples), total=len(input_tuples), desc="å¹¶è¡Œå¤„ç†ä¿¡å·"))

    X_processed = np.array([res[0] for res in results])
    y_labels = np.array([res[1] for res in results])
    valid_mask = y_labels != -1
    X_processed = X_processed[valid_mask]
    y_labels = y_labels[valid_mask]
    print(f"çº¯å‡€æ•°æ®é¢„å¤„ç†å®Œæˆã€‚æ ·æœ¬å½¢çŠ¶: {X_processed.shape}ï¼Œæ ‡ç­¾å½¢çŠ¶: {y_labels.shape}")

    # 3. è®­ç»ƒ VACWGAN (åœ¨æ‰€æœ‰çº¯å‡€æ•°æ®ä¸Šè®­ç»ƒ)
    full_dataset = PreprocessedDataset(X_processed, y_labels)
    full_data_loader = DataLoader(full_dataset, batch_size=64, shuffle=True)
    E_trained, G_trained = train_vacwgan_for_augmentation(full_data_loader, num_classes, DEVICE, VACWGAN_EPOCHS)

    # --- [MODIFIED] ---
    # ç§»é™¤æ ·æœ¬åˆæˆæ­¥éª¤ï¼Œåªè¿”å›çº¯å‡€æ•°æ®å’Œè®­ç»ƒå¥½çš„ GAN
    print(f"çº¯å‡€æ•°æ®åŠ è½½å’Œ GAN è®­ç»ƒå®Œæ¯•ã€‚")
    return X_processed, y_labels, E_trained, G_trained, CLEAN_CLASS_NAMES


# --- è¯¦ç»†æŒ‡æ ‡å‡½æ•° (æ— ä¿®æ”¹) ---
def output_detailed_metrics(y_true, y_pred, class_names, output_filepath=None):
    report_lines = []
    report_lines.append("--- è¯¦ç»†æ€§èƒ½æŠ¥å‘Š ---")
    accuracy = accuracy_score(y_true, y_pred)
    macro_f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)
    macro_precision = precision_score(y_true, y_pred, average='macro', zero_division=0)
    macro_recall = recall_score(y_true, y_pred, average='macro', zero_division=0)
    report_lines.append("\n--- æ•´ä½“å®å¹³å‡æŒ‡æ ‡ ---")
    report_lines.append(f"æ€»ä½“å‡†ç¡®ç‡ (Accuracy): {accuracy:.3f}")
    report_lines.append(f"å®å¹³å‡F1åˆ†æ•° (Macro-F1): {macro_f1:.3f}")
    report_lines.append(f"å®å¹³å‡ç²¾ç¡®ç‡ (Macro-Precision): {macro_precision:.3f}")
    report_lines.append(f"å®å¹³å‡å¬å›ç‡ (Macro-Recall): {macro_recall:.3f}")
    report_lines.append("\n--- å„ç±»åˆ«è¯¦ç»†æŒ‡æ ‡ (Precision, Recall, F1-Score) ---")
    class_report = classification_report(y_true, y_pred, target_names=class_names, digits=3, zero_division=0)
    report_lines.append(class_report)
    cm = confusion_matrix(y_true, y_pred)
    report_lines.append("\n--- æ··æ·†çŸ©é˜µ ---")
    report_lines.append(f"ç±»åˆ«ç´¢å¼•: {CLEAN_LABEL_MAP}")
    report_lines.append("çŸ©é˜µ (è¡Œ: çœŸå®æ ‡ç­¾, åˆ—: é¢„æµ‹æ ‡ç­¾):")
    report_lines.append(str(cm))
    print("\n".join(report_lines))
    if output_filepath:
        try:
            with open(output_filepath, 'w', encoding='utf-8') as f:
                f.write("\n".join(report_lines))
            print(f"\nâœ… è¯¦ç»†æ€§èƒ½æŠ¥å‘Šå·²å¯¼å‡ºè‡³: {output_filepath}")
        except Exception as e:
            print(f"\nâš ï¸ å¯¼å‡ºæ€§èƒ½æŠ¥å‘Šå¤±è´¥: {e}")


# --- [MODIFIED] Optuna ä¼˜åŒ–ç›®æ ‡å‡½æ•° (åœ¨å¾ªç¯å†…å¢å¼ºï¼Œåœ¨çº¯å‡€é›†éªŒè¯) ---
def objective_cv(trial, X_all_pure, y_all_pure, E_trained, G_trained, num_classes, device):
    """
    Optunaä¼˜åŒ–ç›®æ ‡å‡½æ•°ï¼ˆKæŠ˜äº¤å‰éªŒè¯ï¼‰
    [MODIFIED] ä¼ å…¥çº¯å‡€æ•°æ®å’Œè®­ç»ƒå¥½çš„ GAN
    """
    # --- è¶…å‚æ•°æœç´¢ç©ºé—´ (ä¸å˜) ---
    learning_rate = trial.suggest_float('lr', 1e-5, 1e-3, log=True)
    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64])
    dropout_rate = trial.suggest_float('dropout', 0.1, 0.6)
    weight_decay = trial.suggest_float('weight_decay', 1e-6, 1e-3, log=True)
    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'AdamW'])
    max_grad_norm = trial.suggest_float('max_grad_norm', 0.5, 2.0)
    use_focal_loss = trial.suggest_categorical('use_focal', [True, False])
    use_label_smoothing = trial.suggest_categorical('use_ls', [True, False])
    use_center_loss = trial.suggest_categorical('use_center', [True, False])
    focal_alpha = trial.suggest_float('focal_alpha', 0.1, 0.5) if use_focal_loss else 0.25
    focal_gamma = trial.suggest_float('focal_gamma', 1.0, 3.0) if use_focal_loss else 2.0
    label_smoothing_factor = trial.suggest_float('ls_factor', 0.05, 0.2) if use_label_smoothing else 0.1
    center_loss_weight = trial.suggest_float('center_w', 0.001, 0.01, log=True) if use_center_loss else 0.003

    try:
        skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)
        fold_accuracies = []
        global_step = 0

        # --- [MODIFIED] ---
        # æ‹†åˆ†çº¯å‡€æ•°æ®
        for fold_idx, (train_idx, val_idx) in enumerate(skf.split(X_all_pure, y_all_pure)):
            X_train_pure, y_train_pure = X_all_pure[train_idx], y_all_pure[train_idx]
            X_val_pure, y_val_pure = X_all_pure[val_idx], y_all_pure[val_idx]

            # --- [NEW] åœ¨ K-Fold å¾ªç¯å†…éƒ¨è¿›è¡Œæ•°æ®å¢å¼º ---
            X_synthetic_list = [X_train_pure]
            y_synthetic_list = [y_train_pure]
            class_counts_fold = Counter(y_train_pure)

            # æ‰¾åˆ°éœ€è¦å¢å¼ºçš„ç±»åˆ«ï¼ˆåŸºäºå½“å‰è®­ç»ƒæŠ˜ï¼‰
            for label_int in np.unique(y_train_pure):
                if class_counts_fold.get(label_int, 0) < VACWGAN_SAMPLES_PER_CLASS:
                    X_minority = X_train_pure[y_train_pure == label_int]
                    y_minority = y_train_pure[y_train_pure == label_int]

                    num_to_generate = VACWGAN_SAMPLES_PER_CLASS  # (ç®€å•èµ·è§ï¼Œç»Ÿä¸€ç”Ÿæˆ50ä¸ª)

                    X_synth, y_synth = generate_synthetic_samples_from_trained(
                        E_trained, G_trained, X_minority, y_minority, num_to_generate, DEVICE
                    )

                    if X_synth.size > 0:
                        X_synthetic_list.append(X_synth)
                        y_synthetic_list.append(y_synth)

            X_train_aug = np.concatenate(X_synthetic_list, axis=0)
            y_train_aug = np.concatenate(y_synthetic_list, axis=0)
            # --- å¢å¼ºç»“æŸ ---

            # --- [MODIFIED] ---
            # å½’ä¸€åŒ–ï¼šMean/Std å¿…é¡»æ¥è‡ªå¢å¼ºåçš„è®­ç»ƒé›† (X_train_aug)
            current_fold_mean = X_train_aug.mean(axis=(0, 1, 2), keepdims=True)
            current_fold_std = X_train_aug.std(axis=(0, 1, 2), keepdims=True) + 1e-8
            X_train_norm = (X_train_aug - current_fold_mean) / current_fold_std
            # éªŒè¯é›†ä½¿ç”¨è®­ç»ƒé›†çš„ Mean/Stdï¼Œå¹¶ä¸”æ˜¯çº¯å‡€æ•°æ®
            X_val_norm = (X_val_pure - current_fold_mean) / current_fold_std

            model = Simple2DCNN(num_classes=num_classes, dropout=dropout_rate).to(device)
            center_feature_dim = model.center_feature_dim

            # ä½¿ç”¨ TensorDataset (ä¸è‡ªåŠ¨åŠ é€šé“)
            train_dataset = TensorDataset(torch.FloatTensor(X_train_norm), torch.LongTensor(y_train_aug))
            val_dataset = TensorDataset(torch.FloatTensor(X_val_norm), torch.LongTensor(y_val_pure))
            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
            val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

            # æƒé‡è®¡ç®—åº”åŸºäºå¢å¼ºåçš„è®­ç»ƒé›†æ ‡ç­¾
            class_weights = calculate_class_weights(y_train_aug)

            criterion = CombinedLoss(
                num_classes=num_classes, feature_dim=center_feature_dim, use_focal=use_focal_loss,
                use_label_smoothing=use_label_smoothing, use_center_loss=use_center_loss,
                focal_alpha=focal_alpha, focal_gamma=focal_gamma, label_smoothing_factor=label_smoothing_factor,
                center_loss_weight=center_loss_weight, class_weights=class_weights.to(device)
            )

            if optimizer_name == 'Adam':
                optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)
            else:
                optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)

            scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10)
            fold_best_acc = 0

            for epoch in range(NUM_EPOCHS):
                model.train()
                for X_batch, y_batch in train_loader:
                    # æ‰‹åŠ¨æ·»åŠ é€šé“ç»´åº¦ (C=1)ï¼Œå› ä¸ºç”¨çš„æ˜¯ TensorDataset
                    X_batch = X_batch.to(device).unsqueeze(1);
                    y_batch = y_batch.to(device)
                    optimizer.zero_grad()
                    outputs, features = model(X_batch, return_features=True)
                    loss, _ = criterion(outputs, features, y_batch)
                    loss.backward()
                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=max_grad_norm)
                    optimizer.step()

                model.eval()
                val_loss = 0
                val_correct = 0
                val_total = 0
                with torch.no_grad():
                    for X_batch, y_batch in val_loader:
                        # æ‰‹åŠ¨æ·»åŠ é€šé“ç»´åº¦ (C=1)
                        X_batch = X_batch.to(device).unsqueeze(1);
                        y_batch = y_batch.to(device)
                        outputs, features = model(X_batch, return_features=True)
                        val_loss += criterion(outputs, features, y_batch)[0].item() * X_batch.size(0)
                        _, predicted = torch.max(outputs.data, 1)
                        val_total += y_batch.size(0)
                        val_correct += (predicted == y_batch).sum().item()

                # [MODIFIED] éªŒè¯é›†ç°åœ¨æ˜¯çº¯å‡€çš„ï¼Œåˆ†æ•°æ›´çœŸå®
                val_acc = 100 * val_correct / val_total
                val_loss /= val_total
                scheduler.step(val_loss)
                trial.report(val_acc, global_step)

                if val_acc > fold_best_acc:
                    fold_best_acc = val_acc
                if trial.should_prune():
                    raise optuna.exceptions.TrialPruned()
                global_step += 1

            fold_accuracies.append(fold_best_acc)

        mean_acc = np.mean(fold_accuracies)

        # --- [MODIFIED] ---
        # ä¸å†ä¿å­˜æ¨¡å‹çŠ¶æ€åˆ° trial.user_attrsã€‚Optuna åªè´Ÿè´£è¿”å›å¹³å‡ç²¾åº¦ã€‚
        return mean_acc

    except Exception as e:
        print(f"Trial {trial.number} failed with error: {e}")
        return 0.0


# --- [NEW] ç”¨äºè®­ç»ƒæœ€ç»ˆé›†æˆæ¨¡å‹çš„ç‹¬ç«‹å‡½æ•° ---
def train_final_ensemble(X_all_pure, y_all_pure, E_trained, G_trained, best_params, num_classes, optimization_dir):
    """
    ä½¿ç”¨æœ€ä½³è¶…å‚æ•°ï¼Œè®­ç»ƒ N_FOLDS ä¸ªæ¨¡å‹ç”¨äºé›†æˆï¼Œå¹¶ä¿å­˜å®ƒä»¬ã€‚
    """
    print("\n" + "=" * 60)
    print(f"ğŸš€ å¼€å§‹è®­ç»ƒæœ€ç»ˆ {N_FOLDS}-Fold é›†æˆæ¨¡å‹...")
    print("=" * 60)

    skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)

    # ç”¨äºæœ€ç»ˆéªŒè¯çš„æŒ‡æ ‡
    all_val_labels = []
    all_val_preds = []

    # ä» best_params ä¸­æå–å‚æ•°
    lr = best_params['lr']
    batch_size = best_params['batch_size']
    dropout = best_params['dropout']
    weight_decay = best_params['weight_decay']
    optimizer_name = best_params['optimizer']
    max_grad_norm = best_params['max_grad_norm']

    # æå–æŸå¤±å‡½æ•°å‚æ•°
    loss_params = {
        'use_focal': best_params['use_focal'],
        'use_label_smoothing': best_params['use_ls'],
        'use_center_loss': best_params['use_center'],
        'focal_alpha': best_params.get('focal_alpha', 0.25),
        'focal_gamma': best_params.get('focal_gamma', 2.0),
        'label_smoothing_factor': best_params.get('ls_factor', 0.1),
        'center_loss_weight': best_params.get('center_w', 0.003)
    }

    for fold_idx, (train_idx, val_idx) in enumerate(skf.split(X_all_pure, y_all_pure)):
        print(f"\n--- æ­£åœ¨è®­ç»ƒ Fold {fold_idx + 1}/{N_FOLDS} ---")

        X_train_pure, y_train_pure = X_all_pure[train_idx], y_all_pure[train_idx]
        X_val_pure, y_val_pure = X_all_pure[val_idx], y_all_pure[val_idx]

        # 1. æ•°æ®å¢å¼º (ä¸ objective_cv é€»è¾‘ç›¸åŒ)
        X_synthetic_list = [X_train_pure]
        y_synthetic_list = [y_train_pure]
        class_counts_fold = Counter(y_train_pure)
        for label_int in np.unique(y_train_pure):
            if class_counts_fold.get(label_int, 0) < VACWGAN_SAMPLES_PER_CLASS:
                X_minority = X_train_pure[y_train_pure == label_int]
                y_minority = y_train_pure[y_train_pure == label_int]
                num_to_generate = VACWGAN_SAMPLES_PER_CLASS
                X_synth, y_synth = generate_synthetic_samples_from_trained(
                    E_trained, G_trained, X_minority, y_minority, num_to_generate, DEVICE
                )
                if X_synth.size > 0:
                    X_synthetic_list.append(X_synth)
                    y_synthetic_list.append(y_synth)
        X_train_aug = np.concatenate(X_synthetic_list, axis=0)
        y_train_aug = np.concatenate(y_synthetic_list, axis=0)

        # 2. å½’ä¸€åŒ– (ä¸ objective_cv é€»è¾‘ç›¸åŒ)
        current_fold_mean = X_train_aug.mean(axis=(0, 1, 2), keepdims=True)
        current_fold_std = X_train_aug.std(axis=(0, 1, 2), keepdims=True) + 1e-8
        X_train_norm = (X_train_aug - current_fold_mean) / current_fold_std
        X_val_norm = (X_val_pure - current_fold_mean) / current_fold_std

        # 3. ä¿å­˜å½’ä¸€åŒ–ç»Ÿè®¡æ•°æ®
        mean_path = os.path.join(optimization_dir, f"best_model_mean_fold_{fold_idx}.npy")
        std_path = os.path.join(optimization_dir, f"best_model_std_fold_{fold_idx}.npy")
        np.save(mean_path, current_fold_mean)
        np.save(std_path, current_fold_std)
        print(f"Fold {fold_idx} å½’ä¸€åŒ–ç»Ÿè®¡æ•°æ®å·²ä¿å­˜ã€‚")

        # 4. åˆå§‹åŒ–æ¨¡å‹ã€æ•°æ®å’ŒæŸå¤±
        model = Simple2DCNN(num_classes=num_classes, dropout=dropout).to(DEVICE)
        center_feature_dim = model.center_feature_dim
        train_dataset = TensorDataset(torch.FloatTensor(X_train_norm), torch.LongTensor(y_train_aug))
        val_dataset = TensorDataset(torch.FloatTensor(X_val_norm), torch.LongTensor(y_val_pure))
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
        class_weights = calculate_class_weights(y_train_aug)

        criterion = CombinedLoss(
            num_classes=num_classes, feature_dim=center_feature_dim,
            class_weights=class_weights.to(DEVICE), **loss_params
        )
        if optimizer_name == 'Adam':
            optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)
        else:
            optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10)

        # 5. è®­ç»ƒæ¨¡å‹
        fold_best_acc = 0.0
        best_model_state_fold = None

        for epoch in range(NUM_EPOCHS):
            model.train()
            for X_batch, y_batch in train_loader:
                X_batch = X_batch.to(DEVICE).unsqueeze(1);
                y_batch = y_batch.to(DEVICE)
                optimizer.zero_grad()
                outputs, features = model(X_batch, return_features=True)
                loss, _ = criterion(outputs, features, y_batch)
                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)
                optimizer.step()

            model.eval()
            val_loss_epoch = 0;
            val_correct_epoch = 0;
            val_total_epoch = 0
            current_val_labels = [];
            current_val_preds = []

            with torch.no_grad():
                for X_batch, y_batch in val_loader:
                    X_batch = X_batch.to(DEVICE).unsqueeze(1);
                    y_batch = y_batch.to(DEVICE)
                    outputs, features = model(X_batch, return_features=True)
                    val_loss_epoch += criterion(outputs, features, y_batch)[0].item() * X_batch.size(0)
                    _, predicted = torch.max(outputs.data, 1)
                    val_total_epoch += y_batch.size(0)
                    val_correct_epoch += (predicted == y_batch).sum().item()
                    current_val_labels.extend(y_batch.cpu().numpy())
                    current_val_preds.extend(predicted.cpu().numpy())

            val_acc_epoch = 100 * val_correct_epoch / val_total_epoch
            val_loss_epoch /= val_total_epoch
            scheduler.step(val_loss_epoch)

            if val_acc_epoch > fold_best_acc:
                fold_best_acc = val_acc_epoch
                best_model_state_fold = model.state_dict().copy()
                # ä¿å­˜è¿™ä¸€æŠ˜çš„æœ€ä½³é¢„æµ‹ç»“æœ
                all_val_labels_fold = current_val_labels
                all_val_preds_fold = current_val_preds

            if (epoch + 1) % 20 == 0:
                print(
                    f"  Epoch {epoch + 1}/{NUM_EPOCHS}, Val Loss: {val_loss_epoch:.4f}, Val Acc: {val_acc_epoch:.2f}%")

        print(f"Fold {fold_idx} è®­ç»ƒå®Œæ¯•ã€‚æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {fold_best_acc:.3f}%")

        # 6. ä¿å­˜å½“å‰æŠ˜çš„æœ€ä½³æ¨¡å‹
        if best_model_state_fold:
            model_path = os.path.join(optimization_dir, f"best_model_fold_{fold_idx}.pth")
            torch.save(best_model_state_fold, model_path)
            print(f"Fold {fold_idx} æœ€ä½³æ¨¡å‹å·²ä¿å­˜ã€‚")
            all_val_labels.extend(all_val_labels_fold)
            all_val_preds.extend(all_val_preds_fold)
        else:
            print(f"âš ï¸ è­¦å‘Š: Fold {fold_idx} æœªèƒ½ä¿å­˜æ¨¡å‹ (æœªæ‰¾åˆ°æœ€ä½³çŠ¶æ€)ã€‚")

    print(f"\nâœ… {N_FOLDS}-Fold é›†æˆæ¨¡å‹è®­ç»ƒå®Œæ¯•ã€‚")
    return all_val_labels, all_val_preds


# --- [MODIFIED] ä¸»å‡½æ•°ï¼Œæ‰§è¡Œè°ƒä¼˜å’Œé›†æˆè®­ç»ƒ ---
def hyperparameter_optimization():
    """ä¸»å‡½æ•°ï¼Œè´Ÿè´£é©±åŠ¨ GAN è®­ç»ƒã€Optuna è°ƒä¼˜å’Œæœ€ç»ˆé›†æˆæ¨¡å‹è®­ç»ƒ"""
    print("\n" + "=" * 60)
    print("ğŸ” é˜¶æ®µ 1: åŠ è½½çº¯å‡€æ•°æ®å¹¶è®­ç»ƒ VACWGAN")
    print("=" * 60)

    # 1. åŠ è½½çº¯å‡€æ•°æ®å¹¶è®­ç»ƒ GAN (ä¸€æ¬¡æ€§)
    X_all_pure, y_all_pure, E_trained, G_trained, class_names = load_pure_data_and_train_gan()
    num_classes = len(class_names)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    optimization_dir = f"./hyperparameter_optimization_{timestamp}"
    Path(optimization_dir).mkdir(parents=True, exist_ok=True)

    print("\n" + "=" * 60)
    print("ğŸ” é˜¶æ®µ 2: Optuna è¶…å‚æ•°ä¼˜åŒ–")
    print("=" * 60)

    study = optuna.create_study(
        direction='maximize',
        sampler=optuna.samplers.TPESampler(seed=SEED),
        pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=10)
    )

    # Optuna ç›®æ ‡å‡½æ•°éœ€è¦ GAN å’Œçº¯å‡€æ•°æ®
    def objective_wrapper(trial):
        return objective_cv(
            trial, X_all_pure, y_all_pure, E_trained, G_trained, num_classes, DEVICE
        )

    print(f"\nå¼€å§‹ä¼˜åŒ– (å…±{N_TRIALS}æ¬¡è¯•éªŒ)...")
    study.optimize(
        objective_wrapper,
        n_trials=N_TRIALS,
        timeout=OPTIMIZATION_TIMEOUT,
        show_progress_bar=True
    )

    best_trial = study.best_trial
    best_params = best_trial.params

    print(f"\nâœ… ä¼˜åŒ–å®Œæˆ! æœ€ä½³å¹³å‡éªŒè¯å‡†ç¡®ç‡ (æ¥è‡ªOptuna): {study.best_value:.3f}%")
    print("æœ€ä½³å‚æ•°:", best_params)

    # 3. ä¿å­˜æœ€ä½³è¶…å‚æ•°
    best_params_path = os.path.join(optimization_dir, "best_params.json")
    with open(best_params_path, 'w') as f:
        json.dump(best_params, f, indent=4)
    print(f"âœ… æœ€ä½³è¶…å‚æ•°å·²ä¿å­˜è‡³: {best_params_path}")

    # 4. --- [NEW] è®­ç»ƒæœ€ç»ˆçš„ 5-Fold é›†æˆæ¨¡å‹ ---
    all_val_labels, all_val_preds = train_final_ensemble(
        X_all_pure, y_all_pure, E_trained, G_trained, best_params, num_classes, optimization_dir
    )

    # 5. --- [NEW] è¾“å‡ºé›†æˆæ¨¡å‹çš„ K-Fold éªŒè¯æ€§èƒ½ ---
    if all_val_labels and all_val_preds:
        print("\n" + "=" * 60)
        print(f"ğŸ“Š {N_FOLDS}-Fold é›†æˆæ¨¡å‹åœ¨ [çº¯å‡€éªŒè¯é›†] ä¸Šçš„èšåˆæ€§èƒ½æŠ¥å‘Š")
        print("=" * 60)
        val_report_path = os.path.join(optimization_dir, "ensemble_validation_performance_report.txt")
        output_detailed_metrics(np.array(all_val_labels), np.array(all_val_preds), class_names, val_report_path)
    else:
        print("\nâš ï¸ æœªèƒ½ç”Ÿæˆé›†æˆæ¨¡å‹çš„éªŒè¯æŠ¥å‘Š (æ²¡æœ‰æ”¶é›†åˆ°æ ‡ç­¾æˆ–é¢„æµ‹)ã€‚")

    return best_params, optimization_dir, class_names


# --- [MODIFIED] æµ‹è¯•é›†æ¨ç†å‡½æ•° (æ”¯æŒé›†æˆ) ---
def run_test_inference(optimization_dir, class_names, best_params):
    """åŠ è½½ 5-Fold é›†æˆæ¨¡å‹ï¼Œå¤„ç†æœ€ç»ˆæµ‹è¯•é›†ï¼Œå¹¶è¾“å‡º TXT æ ¼å¼ç»“æœ"""

    num_classes = len(class_names)
    dropout_rate = best_params.get('dropout', 0.5)

    # 1. åŠ è½½ 5-Fold é›†æˆæ¨¡å‹
    models_list = []
    stats_list = []
    print("\n--- åŠ è½½ 5-Fold é›†æˆæ¨¡å‹åŠç»Ÿè®¡æ•°æ® ---")

    for i in range(N_FOLDS):
        try:
            model = Simple2DCNN(num_classes=num_classes, dropout=dropout_rate).to(DEVICE)
            model_path = os.path.join(optimization_dir, f"best_model_fold_{i}.pth")
            model.load_state_dict(torch.load(model_path, map_location=DEVICE))
            model.eval()
            models_list.append(model)

            mean_path = os.path.join(optimization_dir, f"best_model_mean_fold_{i}.npy")
            std_path = os.path.join(optimization_dir, f"best_model_std_fold_{i}.npy")
            data_mean = np.load(mean_path)
            data_std = np.load(std_path)
            stats_list.append({'mean': data_mean, 'std': data_std})

            print(f"æˆåŠŸåŠ è½½ Fold {i} æ¨¡å‹åŠç»Ÿè®¡æ•°æ®ã€‚")
        except FileNotFoundError as e:
            print(f"\né”™è¯¯: æœªæ‰¾åˆ° Fold {i} çš„æ¨¡å‹æˆ–ç»Ÿè®¡æ–‡ä»¶: {e}")
            print("è¯·ç¡®ä¿å·²æˆåŠŸè¿è¡Œ train_final_ensembleã€‚")
            return

    if not models_list:
        print("é”™è¯¯ï¼šæœªèƒ½åŠ è½½ä»»ä½•æ¨¡å‹ï¼Œæ¨ç†ä¸­æ­¢ã€‚")
        return

    # 2. é¢„å¤„ç†æµ‹è¯•é›†æ•°æ® (å¹¶è¡Œ)
    test_files = [f for f in os.listdir(TEST_ROOT_DIR) if f.endswith(".xlsx")]
    final_test_paths = [os.path.join(TEST_ROOT_DIR, f) for f in test_files]
    inference_input_paths = final_test_paths

    print(f"\nå¼€å§‹å¤„ç†æœ€ç»ˆæµ‹è¯•é›†æ•°æ® (å¹¶è¡Œ, {cpu_count()} æ ¸å¿ƒ)...")
    with Pool(cpu_count()) as pool:
        X_final_test_processed = list(
            tqdm(pool.imap(process_single_test_signal_worker, inference_input_paths), total=len(inference_input_paths),
                 desc="å¹¶è¡Œå¤„ç†æµ‹è¯•ä¿¡å·"))
    X_final_test_processed = np.array(X_final_test_processed)  # å½¢çŠ¶ (N_test, H, W)

    # 3. è¿è¡Œé›†æˆæ¨ç†
    # ç´¯åŠ  logits
    all_ensemble_logits = np.zeros((len(X_final_test_processed), num_classes))

    print("\n--- è¿è¡Œ 5-Fold é›†æˆæ¨ç† ---")
    for i in range(N_FOLDS):
        model = models_list[i]
        stats = stats_list[i]

        # ä½¿ç”¨å½“å‰æŠ˜çš„ç»Ÿè®¡æ•°æ®å½’ä¸€åŒ–
        X_final_test_norm_i = (X_final_test_processed - stats['mean']) / stats['std']

        # PreprocessedDataset ä¼šè‡ªåŠ¨æ·»åŠ é€šé“ç»´åº¦
        final_test_dataset_i = PreprocessedDataset(X_final_test_norm_i, [0] * len(X_final_test_norm_i))
        final_test_loader_i = DataLoader(final_test_dataset_i, batch_size=64, shuffle=False)

        model_logits = []
        with torch.no_grad():
            for inputs, _ in final_test_loader_i:
                inputs = inputs.to(DEVICE)  # (B, 1, H, W)
                logits = model(inputs)
                model_logits.append(logits.cpu().numpy())

        model_logits_np = np.concatenate(model_logits, axis=0)
        all_ensemble_logits += model_logits_np
        print(f"Fold {i} æ¨ç†å®Œæˆã€‚")

    # 4. è®¡ç®—å¹³å‡ Logits å¹¶è·å–æœ€ç»ˆé¢„æµ‹
    avg_logits = all_ensemble_logits / N_FOLDS
    y_pred_final = np.argmax(avg_logits, axis=1)

    # 5. å¯¼å‡ºé¢„æµ‹ç»“æœ (ä½¿ç”¨ CLEAN_CLASS_NAMES)
    output_filename = "test_prediction_results.txt"
    with open(output_filename, 'w', encoding='utf-8') as f:
        f.write("æµ‹è¯•é›†åç§°\tæ•…éšœç±»å‹\n")
        for path, pred_index in zip(final_test_paths, y_pred_final):
            base_name = os.path.basename(path)
            test_set_name = os.path.splitext(base_name)[0]
            predicted_fault_type = class_names[pred_index]
            f.write(f"{test_set_name}\t{predicted_fault_type}\n")

    print(f"\nâœ… æµ‹è¯•é›†é¢„æµ‹ç»“æœå·²å¯¼å‡ºè‡³: '{output_filename}'")
    pred_counts = Counter(y_pred_final)
    print(f"\n--- æµ‹è¯•é›†é¢„æµ‹åˆ†å¸ƒæ¦‚è§ˆ (å…± {len(y_pred_final)} ä¸ªæ ·æœ¬) ---")
    for pred_idx, count in pred_counts.items():
        print(f"  {class_names[pred_idx]:<20}: {count} ä¸ª")


if __name__ == '__main__':
    # --- é˜¶æ®µ 1 & 2 & 3: GANè®­ç»ƒã€è¶…å‚æ•°ä¼˜åŒ–ã€é›†æˆæ¨¡å‹è®­ç»ƒ ---
    best_params, optimization_dir, class_names = hyperparameter_optimization()

    # --- é˜¶æ®µ 4: æœ€ç»ˆæµ‹è¯•é›†é›†æˆæ¨ç† ---
    print("\n" + "=" * 60)
    print("ğŸš€ è¿è¡Œæœ€ç»ˆæµ‹è¯•é›†é›†æˆæ¨ç†")
    print("=" * 66)
    run_test_inference(optimization_dir, class_names, best_params)