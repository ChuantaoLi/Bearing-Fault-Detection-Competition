#!/usr/bin/env python3
"""
*** ä¿®æ”¹ç‰ˆ ***
åŠ è½½ 'augmented_dataset_5fold.pkl'ã€‚
Optuna è°ƒä¼˜ï¼šä½¿ç”¨ pkl æ–‡ä»¶ä¸­é¢„å…ˆåˆ’åˆ†çš„ 5 æŠ˜æ•°æ®è¿›è¡Œäº¤å‰éªŒè¯ã€‚
æœ€ç»ˆè®­ç»ƒï¼šä½¿ç”¨ç¬¬ 1 æŠ˜çš„æ•°æ®è¿›è¡Œè®­ç»ƒ/éªŒè¯åˆ’åˆ†ï¼Œè®­ç»ƒæœ€ç»ˆæ¨¡å‹ã€‚
æœ€ç»ˆé¢„æµ‹ï¼šä½¿ç”¨æœ€ç»ˆæ¨¡å‹å¯¹ pkl æ–‡ä»¶ä¸­çš„å…¨å±€æµ‹è¯•é›†è¿›è¡Œé¢„æµ‹ã€‚
"""

import os
import pickle
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.metrics import classification_report, confusion_matrix
from pathlib import Path
from datetime import datetime
import matplotlib.pyplot as plt
import seaborn as sns
import json
import optuna
from collections import Counter
import shutil

# ==================== é…ç½®å‚æ•° ====================
# *** ä¿®æ”¹ç‚¹: æŒ‡å‘æ‚¨æ–°ç”Ÿæˆçš„ã€åŒ…å«5æŠ˜æ•°æ®çš„æ–‡ä»¶ ***
OPTIMIZED_DATASET_PATH = r"augmented_dataset_5fold.pkl"

"""
æ¯ä¸ªé¢‘è°±æ ·æœ¬ = 7æ¡æ—¶é—´åºåˆ—
    â†“
åºåˆ—1: ä½é¢‘èƒ½é‡éšæ—¶é—´å˜åŒ– [å€¼1, å€¼2, ..., å€¼48]
åºåˆ—2: ä¸­ä½é¢‘èƒ½é‡éšæ—¶é—´å˜åŒ– [å€¼1, å€¼2, ..., å€¼48]
...
åºåˆ—7: é«˜é¢‘èƒ½é‡éšæ—¶é—´å˜åŒ– [å€¼1, å€¼2, ..., å€¼48]

æ¯ä¸ªåŒ…ç»œæ ·æœ¬ = 5æ¡æ—¶é—´åºåˆ—  
    â†“
åºåˆ—1: åŒ…ç»œç‰¹å¾1éšæ—¶é—´å˜åŒ– [å€¼1, å€¼2, ..., å€¼32]
åºåˆ—2: åŒ…ç»œç‰¹å¾2éšæ—¶é—´å˜åŒ– [å€¼1, å€¼2, ..., å€¼32]
...
åºåˆ—5: åŒ…ç»œç‰¹å¾5éšæ—¶é—´å˜åŒ– [å€¼1, å€¼2, ..., å€¼32]
"""

# è¶…å‚æ•°ä¼˜åŒ–é…ç½®
ENABLE_HYPERPARAMETER_TUNING = True  # æ˜¯å¦å¯ç”¨è¶…å‚æ•°ä¼˜åŒ–
N_TRIALS = 100  # Optunaè¯•éªŒæ¬¡æ•°
OPTIMIZATION_TIMEOUT = 7200  # ä¼˜åŒ–è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

# è®­ç»ƒå‚æ•°ï¼ˆä¼˜åŒ–æ—¶ä¼šè¢«è¦†ç›–ï¼‰
NUM_EPOCHS = 100
BATCH_SIZE = 32
LEARNING_RATE = 0.0005
EARLY_STOPPING_PATIENCE = 40
DROPOUT_RATE = 0.4

# è®¾ç½®éšæœºç§å­
np.random.seed(42)
torch.manual_seed(42)
if torch.cuda.is_available():
    torch.cuda.manual_seed(42)


# ==================== è¾…åŠ©å‡½æ•° ====================
def calculate_class_weights(y_train):
    """è®¡ç®—ç±»åˆ«æƒé‡ç”¨äºå¤„ç†ç±»åˆ«ä¸å¹³è¡¡"""
    class_counts = Counter(y_train)
    num_classes = len(class_counts)

    # ç¡®ä¿æˆ‘ä»¬çŸ¥é“æ€»ç±»åˆ«æ•°, å³ä½¿æŸä¸ªç±»åˆ«åœ¨y_trainä¸­ä¸å­˜åœ¨
    max_class_id = 0
    if class_counts:  # ç¡®ä¿
        max_class_id = max(class_counts.keys())
    total_num_classes = max(num_classes, max_class_id + 1)

    total_samples = len(y_train)

    # è½¬æ¢ä¸ºtensor
    weights = torch.zeros(total_num_classes)
    for class_id, count in class_counts.items():
        if count > 0:
            weights[class_id] = total_samples / (total_num_classes * count)
        else:
            weights[class_id] = 1.0  # ç†è®ºä¸Šä¸åº”å‘ç”Ÿ

    # å¤„ç† y_train ä¸­å¯èƒ½ç¼ºå¤±çš„ç±»åˆ«
    for i in range(total_num_classes):
        if i not in class_counts:
            weights[i] = 1.0  # å¦‚æœæŸä¸ªç±»åˆ«å®Œå…¨ç¼ºå¤±ï¼Œç»™ä¸€ä¸ªä¸­æ€§æƒé‡

    return weights


# ==================== æŸå¤±å‡½æ•°å®šä¹‰ ====================
class FocalLoss(nn.Module):
    """
    Focal Lossç”¨äºå¤„ç†ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜

    ç”¨ä¸€ä¸ªä¾‹å­æ¼”ç¤ºè¿™ä¸ªè®¡ç®—è¿‡ç¨‹ï¼š

    1. å‡è®¾æ‰¹æ¬¡ä¸º3ï¼Œæœ‰å¦‚ä¸‹3ä¸ªæ ·æœ¬ï¼š
    æ ·æœ¬1: [3.0, 1.0, 0.5, -0.5, 1.5, 0.0]   â†’ çœŸå®ç±»åˆ«: 0
    æ ·æœ¬2: [0.5, 2.5, 1.0, -1.0, 0.0, 0.5]   â†’ çœŸå®ç±»åˆ«: 1
    æ ·æœ¬3: [1.0, 0.5, 2.0, -0.5, 1.0, -1.0]  â†’ çœŸå®ç±»åˆ«: 2

    2. è®¡ç®—Softmaxæ¦‚ç‡ï¼š
    æ ·æœ¬1: softmax([3.0, 1.0, 0.5, -0.5, 1.5, 0.0])
      = [0.830, 0.112, 0.037, 0.005, 0.015, 0.001]
      â†’ çœŸå®ç±»åˆ«0çš„æ¦‚ç‡ pâ‚€ = 0.830
      â†’ ce_loss = -log(0.830) = 0.186
    å…¶ä½™ä¿©æ ·æœ¬åŒç†ï¼Œæœ€ç»ˆå¾—åˆ°ï¼šce_loss = [0.186, 0.408, 0.245]

    3. è®¡ç®—æ¦‚ç‡é¡¹ (pt)
    pt å¹¶ä¸æ˜¯"çœŸå®ç±»åˆ«çš„æ¦‚ç‡"ï¼Œè€Œæ˜¯"æ¨¡å‹é¢„æµ‹æ­£ç¡®çš„æ¦‚ç‡"çš„æ•°å­¦è¡¨è¾¾ã€‚
    pt = torch.exp(-ce_loss)
    æ ·æœ¬1: pt = exp(-0.186) = 0.830  (å°±æ˜¯çœŸå®ç±»åˆ«çš„æ¦‚ç‡pâ‚€)
    æ ·æœ¬2: pt = exp(-0.408) = 0.665  (å°±æ˜¯çœŸå®ç±»åˆ«çš„æ¦‚ç‡pâ‚)
    æ ·æœ¬3: pt = exp(-0.245) = 0.783  (å°±æ˜¯çœŸå®ç±»åˆ«çš„æ¦‚ç‡pâ‚‚)

    4. è®¡ç®—è°ƒèŠ‚å› å­ (1-pt)^Î³
    æ ·æœ¬1: (1 - 0.830)Â² = (0.170)Â² = 0.0289
    æ ·æœ¬2: (1 - 0.665)Â² = (0.335)Â² = 0.1122
    æ ·æœ¬3: (1 - 0.783)Â² = (0.217)Â² = 0.0471

    5. åº”ç”¨alphaå¹³è¡¡å› å­
    focal_loss = alpha Ã— (1-pt)^Î³ Ã— ce_loss
    æ ·æœ¬1: 0.25 Ã— 0.0289 Ã— 0.186 = 0.00134
    æ ·æœ¬2: 0.25 Ã— 0.1122 Ã— 0.408 = 0.01144
    æ ·æœ¬3: 0.25 Ã— 0.0471 Ã— 0.245 = 0.00288

    6. æœ€ç»ˆæŸå¤±
    focal_loss = [0.00134, 0.01144, 0.00288]
    å‡å€¼: (0.00134 + 0.01144 + 0.00288) / 3 = 0.00522
    """

    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean', class_weights=None):
        super(FocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction
        self.class_weights = class_weights

        '''
        alpha (é»˜è®¤0.25): å¹³è¡¡å› å­ï¼Œç”¨äºè°ƒèŠ‚æ­£è´Ÿæ ·æœ¬çš„æƒé‡
        gamma (é»˜è®¤2.0): è°ƒèŠ‚å› å­ï¼Œæ§åˆ¶éš¾æ˜“æ ·æœ¬çš„æƒé‡
        reduction (é»˜è®¤'mean'): æŸå¤± reduction æ–¹å¼ï¼Œå¯é€‰ 'mean'ã€'sum'ã€'none'
        class_weights: é¢å¤–çš„ç±»åˆ«æƒé‡ï¼Œç”¨äºè¿›ä¸€æ­¥å¤„ç†ç±»åˆ«ä¸å¹³è¡¡
        '''

    def forward(self, inputs, targets):
        """è®¡ç®—äº¤å‰ç†µæŸå¤±"""
        ce_loss = F.cross_entropy(inputs, targets, reduction='none')
        pt = torch.exp(-ce_loss)

        # åº”ç”¨ç±»åˆ«æƒé‡
        if self.class_weights is not None:
            # ç¡®ä¿ class_weights åœ¨åŒä¸€è®¾å¤‡ä¸Š
            if self.class_weights.device != targets.device:
                self.class_weights = self.class_weights.to(targets.device)
            weights = self.class_weights[targets]
            ce_loss = ce_loss * weights

        """
        æŸå¤±å‡½æ•°ï¼šFL(pt) = Î± Ã— (1 - pt)^Î³ Ã— CE(pt)
        (1 - pt)^Î³æ˜¯è°ƒèŠ‚å› å­ï¼Œå¯¹äºæ˜“åˆ†ç±»æ ·æœ¬ (pt â†’ 1)ï¼Œæ­¤é¡¹è¶‹è¿‘äº0ï¼Œæƒé‡é™ä½ï¼Œgammaè¶Šå¤§ï¼Œå¯¹æ˜“åˆ†ç±»æ ·æœ¬çš„æŠ‘åˆ¶è¶Šå¼º
        Î±æ˜¯å¹³è¡¡å› å­ï¼Œç”¨æ¥ç»™æ­£è´Ÿç±»å¹³è¡¡ï¼Œå¤šåˆ†ç±»å¯ä»¥çœ‹ä½œæ˜¯ç›®æ ‡ç±»å’Œéç›®æ ‡ç±»ï¼Œæ‰€ä»¥alphaç”¨æ ‡é‡ä¹Ÿæ˜¯åˆç†çš„
        CE(pt)æ˜¯ç±»åˆ«æƒé‡ï¼Œåœ¨å‰é¢æ ¹æ®ç±»åˆ«ä¸å¹³è¡¡ç‡è¿›è¡Œè®¡ç®—
        """

        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss

        """å¯¹ä¸€ä¸ªæ‰¹æ¬¡å†…çš„æ ·æœ¬çš„focal lossè¿›è¡Œç¼©å‡"""
        if self.reduction == 'mean':
            return focal_loss.mean()
        elif self.reduction == 'sum':
            return focal_loss.sum()
        else:
            return focal_loss


class LabelSmoothingCrossEntropy(nn.Module):
    """
    æ ‡ç­¾å¹³æ»‘äº¤å‰ç†µæŸå¤±

    ç”¨ä¸€ä¸ªä¾‹å­æ¼”ç¤ºè®¡ç®—è¿‡ç¨‹ï¼š

    1. å‡è®¾è¾“å‡ºçš„æ•°æ® logits (x) å¦‚ä¸‹ï¼š
        tensor([[3.0000, 1.0000, 0.5000, 0.2000],
            [0.5000, 2.5000, 1.0000, 0.1000],
            [1.0000, 0.5000, 2.5000, 0.3000]], requires_grad=True)
        å…¶çœŸå®æ ‡ç­¾ä¸º: tensor([0, 1, 2])

    2. è®¾å®šå¹³æ»‘å‚æ•° smoothing: 0.1ï¼Œç½®ä¿¡åº¦ confidence: 0.9

    3. è®¡ç®—Softmaxæ¦‚ç‡åˆ†å¸ƒ:
        æ ·æœ¬0: [0.8307 0.1125 0.0373 0.0195] (æ€»å’Œ: 1.0000)
        æ ·æœ¬1: [0.0905 0.6652 0.2227 0.0216] (æ€»å’Œ: 1.0000)
        æ ·æœ¬2: [0.1185 0.064 0.7829 0.0346] (æ€»å’Œ: 1.0000)

    4. è®¡ç®—å¯¹æ•°æ¦‚ç‡ logprobs:
        æ ·æœ¬0: [-0.1855 -2.1846 -2.9875 -3.6376]
        æ ·æœ¬1: [-2.4024 -0.4076 -1.4979 -3.835  ]
        æ ·æœ¬2: [-2.1328 -2.7489 -0.2448 -3.3629]

    5. æå–çš„çœŸå®ç±»åˆ«å¯¹æ•°æ¦‚ç‡
        ç›®æ ‡æ ‡ç­¾è°ƒæ•´å½¢çŠ¶: target.unsqueeze(1): [[0], [1], [2]]
        å½¢çŠ¶: torch.Size([3, 1])
        gathered_logprobs: [[-0.18546521663665771], [-0.4076051712036133], [-0.24478435516357422]]

        è´Ÿå¯¹æ•°ä¼¼ç„¶æŸå¤± (before squeeze): [[0.18546521663665771], [0.4076051712036133], [0.24478435516357422]]
        è´Ÿå¯¹æ•°ä¼¼ç„¶æŸå¤± (after squeeze): [0.18546521663665771, 0.4076051712036133, 0.24478435516357422]
        å½¢çŠ¶: torch.Size([3])

    6. è®¡ç®—å¹³æ»‘æŸå¤±
        å¹³æ»‘æŸå¤±: [2.248800039291382, 2.0357494354248047, 2.1220998764038086]
        å½¢çŠ¶: torch.Size([3])

    7. ç»„åˆæŸå¤±
        æŸå¤±ç»„åˆå…¬å¼: loss = confidence Ã— nll_loss + smoothing Ã— smooth_loss

        æ ·æœ¬0: 0.9Ã—0.1855 + 0.1Ã—2.2488 = 0.3928
        æ ·æœ¬1: 0.9Ã—0.4076 + 0.1Ã—2.0357 = 0.5703
        æ ·æœ¬2: 0.9Ã—0.2448 + 0.1Ã—2.1221 = 0.4325

        é€æ ·æœ¬æŸå¤±: [0.392765074968338, 0.5703092217445374, 0.43250468373298645]

    8. æœ€ç»ˆæŸå¤±
        æ‰¹é‡å¹³å‡æŸå¤±: 0.4652
    """

    def __init__(self, smoothing=0.1):
        super(LabelSmoothingCrossEntropy, self).__init__()
        self.smoothing = smoothing

    def forward(self, logits, target):
        confidence = 1. - self.smoothing  # æ ‡ç­¾çš„ç½®ä¿¡åº¦ç­‰äº1å‡å»å¹³æ»‘åº¦ï¼Œå¹³æ»‘åº¦é»˜è®¤ä¸º1
        logprobs = F.log_softmax(logits, dim=-1)  # å¯¹æ¨¡å‹æœ€åä¸€å±‚çš„è¾“å‡ºè¿›è¡Œsoftmaxå½’ä¸€åŒ–

        """
        å¯¹äºlogits = torch.tensor([[2.0, 1.0, 0.1]])ï¼Œå°†logitsè½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒ
        ç»“æœä¼šæ˜¯ tensor([[0.6590, 0.2424, 0.0986]])ï¼Œæ»¡è¶³: 0.6590 + 0.2424 + 0.0986 â‰ˆ 1.0
        å¯¹æ¦‚ç‡å–è‡ªç„¶å¯¹æ•°ï¼šlogprobs = torch.log(probs)
        ç»“æœä¼šæ˜¯ tensor([[-0.4170, -1.4170, -2.2170]])
        """

        nll_loss = -logprobs.gather(dim=-1, index=target.unsqueeze(1))
        nll_loss = nll_loss.squeeze(1)

        """
        ä¸Šé¢è¿™ä¸¤è¡Œä»£ç çš„ä½œç”¨æ˜¯ï¼šä»æ¯ä¸ªæ ·æœ¬çš„æ¦‚ç‡åˆ†å¸ƒä¸­ï¼Œåªæå–çœŸå®ç±»åˆ«å¯¹åº”çš„æŸå¤±å€¼
        target.unsqueeze(1): å°†ç›®æ ‡æ ‡ç­¾ä»å½¢çŠ¶ [batch_size] å˜ä¸º [batch_size, 1]
        logprobs.gather(): ä»logprobsä¸­æå–å¯¹åº”ç›®æ ‡æ ‡ç­¾ä½ç½®çš„å¯¹æ•°æ¦‚ç‡
        -logprobs.gather(): å¾—åˆ°è´Ÿå¯¹æ•°ä¼¼ç„¶æŸå¤±
        squeeze(1): å°†å½¢çŠ¶ä» [batch_size, 1] å˜å› [batch_size]
        gatheråä¼šå¾—åˆ°å·®ä¸å¤šè¿™æ ·çš„å½¢å¼ï¼š
        tensor([[-0.4170],   # æ ·æœ¬1çš„ç±»åˆ«0çš„å¯¹æ•°æ¦‚ç‡
         [-1.6000],   # æ ·æœ¬2çš„ç±»åˆ«2çš„å¯¹æ•°æ¦‚ç‡  
         [-1.7000]])  # æ ·æœ¬3çš„ç±»åˆ«1çš„å¯¹æ•°æ¦‚ç‡
        """

        smooth_loss = -logprobs.mean(dim=-1)  # è®¡ç®—æ ·æœ¬æ‰€æœ‰ç±»åˆ«çš„å¹³å‡æŸå¤±ï¼Œä½œä¸ºæ­£åˆ™åŒ–é¡¹
        loss = confidence * nll_loss + self.smoothing * smooth_loss

        """
        ä¸Šé¢è¿™ä¸¤è¡Œä»£ç æ˜¯æ ‡ç­¾å¹³æ»‘çš„æ ¸å¿ƒ
        smooth_loosä¼šå·®ä¸å¤šæ˜¯ä¸‹é¢è¿™ä¸ªå½¢å¼ï¼š
        æ ·æœ¬1: -(-0.4170 + -1.4170 + -2.3170 + -1.9170)/4 = -(-6.068)/4 = 1.517
        æ ·æœ¬2: -(-2.1000 + -0.1000 + -1.6000 + -2.4000)/4 = -(-6.200)/4 = 1.550
        æ ·æœ¬3: -(-1.2000 + -1.7000 + -0.2000 + -1.9000)/4 = -(-5.000)/4 = 1.250
        """

        return loss.mean()


class CenterLoss(nn.Module):
    """Center Lossç”¨äºå¢å¼ºç‰¹å¾çš„ç±»å†…ç´§å¯†åº¦"""

    def __init__(self, num_classes, feature_dim, use_gpu=True):
        super(CenterLoss, self).__init__()
        self.num_classes = num_classes
        self.feature_dim = feature_dim
        self.use_gpu = use_gpu

        """
        ä¸­å¿ƒå‘é‡æ˜¯Center Lossä¸­ä¸ºæ¯ä¸ªç±»åˆ«å­¦ä¹ çš„ä¸€ä¸ªä»£è¡¨æ€§ç‰¹å¾å‘é‡ï¼Œå¯ä»¥ç†è§£ä¸ºè¯¥ç±»åˆ«çš„"å¹³å‡ç‰¹å¾"æˆ–"ç†æƒ³ç‰¹å¾"
        ä¸­å¿ƒå‘é‡æ˜¯æ¨¡å‹çš„å¯å­¦ä¹ å‚æ•°ï¼Œä¸æ˜¯è®¡ç®—å¾—åˆ°çš„ï¼Œè€Œæ˜¯åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é€šè¿‡æ¢¯åº¦ä¸‹é™è‡ªåŠ¨å­¦ä¹ çš„
        """

        # *** ä¿®æ”¹ç‚¹: å¢åŠ CUDAå¯ç”¨æ€§æ£€æŸ¥ ***
        if self.use_gpu and torch.cuda.is_available():
            self.centers = nn.Parameter(torch.randn(self.num_classes, self.feature_dim).cuda())
        else:
            self.centers = nn.Parameter(torch.randn(self.num_classes, self.feature_dim))
            if self.use_gpu:
                print("è­¦å‘Š: CenterLoss è¯·æ±‚ GPU ä½† CUDA ä¸å¯ç”¨, è‡ªåŠ¨é™çº§åˆ° CPUã€‚")
                self.use_gpu = False

    def forward(self, x, labels):
        batch_size = x.size(0)
        distmat = torch.pow(x, 2).sum(dim=1, keepdim=True).expand(batch_size, self.num_classes) + \
                  torch.pow(self.centers, 2).sum(dim=1, keepdim=True).expand(self.num_classes, batch_size).t()
        distmat.addmm_(x, self.centers.t(), beta=1, alpha=-2)

        """
        x_norm = torch.pow(x, 2).sum(dim=1, keepdim=True)ï¼šå¯¹äºæ‰¹æ¬¡ä¸­æ¯ä¸ªæ ·æœ¬iï¼Œè®¡ç®—å…¶ç‰¹å¾å‘é‡çš„å¹³æ–¹å’Œï¼Œç»“æœå½¢çŠ¶ä¸º [batch_size, 1]
        x_norm_expanded = x_norm.expand(batch_size, self.num_classes)ï¼šå°†æ¯ä¸ªæ ·æœ¬çš„å¹³æ–¹å’Œæ‰©å±•ä¸ºä¸ç±»åˆ«æ•°ç›¸åŒçš„åˆ—æ•°ï¼Œç»“æœå½¢çŠ¶ä¸º [batch_size, num_classes]

        å‡è®¾x_normä¸ºï¼š
            tensor([[5.],   # 1Â² + 2Â² = 1 + 4 = 5
            [10.],  # 3Â² + 1Â² = 9 + 1 = 10
            [6.5]]) # 0.5Â² + 2.5Â² = 0.25 + 6.25 = 6.5
        é‚£ä¹ˆæ‹“å±•åçš„x_norm_expandedå°±ä¸ºï¼š
            tensor([[5., 5., 5., 5.],
            [10., 10., 10., 10.],
            [6.5, 6.5, 6.5, 6.5]])
        expand() å‡½æ•°å°† [3, 1] çš„å¼ é‡æ²¿ç€ç¬¬1ç»´åº¦å¤åˆ¶4æ¬¡ï¼Œè¿™æ ·æ¯è¡Œéƒ½å˜æˆäº†ç›¸åŒçš„4ä¸ªå€¼ã€‚

        center_norm = torch.pow(self.centers, 2).sum(dim=1, keepdim=True)ï¼šå¯¹äºæ¯ä¸ªç±»åˆ«jï¼Œè®¡ç®—å…¶ä¸­å¿ƒå‘é‡çš„å¹³æ–¹å’Œï¼Œç»“æœå½¢çŠ¶ä¸º [num_classes, 1]
        center_norm_expanded = center_norm.expand(self.num_classes, batch_size).t()ï¼šå°†æ¯ä¸ªç±»åˆ«çš„å¹³æ–¹å’Œæ‰©å±•ä¸ºä¸æ‰¹æ¬¡å¤§å°ç›¸åŒçš„è¡Œæ•°ï¼Œå¹¶è½¬ç½®ï¼Œç»“æœå½¢çŠ¶ä¸º [batch_size, num_classes]
        dot_product = torch.mm(x, self.centers.t())ï¼šè®¡ç®—æ‰¹æ¬¡ä¸­æ¯ä¸ªæ ·æœ¬ä¸æ¯ä¸ªç±»åˆ«ä¸­å¿ƒçš„ç‚¹ç§¯ï¼Œç»“æœå½¢çŠ¶ä¸º [batch_size, num_classes]
        """

        classes = torch.arange(self.num_classes).long()  # åˆ›å»ºç±»åˆ«ç´¢å¼• [0, 1, 2, ..., num_classes-1]
        # *** ä¿®æ”¹ç‚¹: ç¡®ä¿è®¾å¤‡ä¸€è‡´æ€§ ***
        if self.use_gpu and torch.cuda.is_available():
            classes = classes.cuda()

        # ç¡®ä¿ labels å’Œ classes åœ¨åŒä¸€è®¾å¤‡ä¸Š
        if labels.device != classes.device:
            classes = classes.to(labels.device)

        labels_expanded = labels.unsqueeze(1).expand(batch_size, self.num_classes)  # æ‹“å±•æ ‡ç­¾å½¢çŠ¶

        """
        å‡è®¾ labels = [0, 1, 2], æ‰©å±•å:
        [[0, 0, 0],
        [1, 1, 1], 
        [2, 2, 2]]
        è¿™é‡Œçš„[0, 1, 2]è¡¨ç¤ºç¬¬ä¸€ä¸ªæ ·æœ¬çš„çœŸå®æ ‡ç­¾æ˜¯0ï¼Œç¬¬äºŒä¸ªæ ·æœ¬çš„çœŸå®æ ‡ç­¾æ˜¯1ï¼Œç¬¬ä¸‰ä¸ªæ ·æœ¬çš„çœŸå®æ ‡ç­¾æ˜¯2
        """

        mask = labels_expanded.eq(classes.expand(batch_size, self.num_classes))  # åˆ›å»ºæ©ç 

        """
        [[True, False, False],
        [False, True, False],
        [False, False, True]]
        """

        dist = distmat * mask.float()  # dist çš„ å½¢çŠ¶æ˜¯ [batch_size, num_classes]
        loss = dist.clamp(min=1e-12, max=1e+12).sum() / batch_size  # loss æ˜¯æ ‡é‡

        """
        æ©ç ç”¨äºåªé€‰æ‹©æ¯ä¸ªæ ·æœ¬åˆ°å…¶çœŸå®ç±»åˆ«ä¸­å¿ƒçš„è·ç¦»
        è®¡ç®—å¾—åˆ°çš„distå½¢å¼å¤§æ¦‚æ˜¯è¿™æ ·çš„ï¼š
            tensor([[1.0, 0.0, 0.0, 0.0],   # åªä¿ç•™æ ·æœ¬0åˆ°ç±»åˆ«0çš„è·ç¦»
            [0.0, 1.5, 0.0, 0.0],   # åªä¿ç•™æ ·æœ¬1åˆ°ç±»åˆ«1çš„è·ç¦»  
            [0.0, 0.0, 1.0, 0.0]])  # åªä¿ç•™æ ·æœ¬2åˆ°ç±»åˆ«2çš„è·ç¦»
        lossæ˜¯é¦–å…ˆå¯¹distçš„æ‰€æœ‰å…ƒç´ æ±‚å’Œï¼Œç„¶åé™¤ä»¥batch_size
        clampæ˜¯ç”¨æ¥é™åˆ¶distæ•°å€¼çš„ï¼Œé˜²æ­¢æå€¼çš„å½±å“
        """

        return loss


class CombinedLoss(nn.Module):
    """ç»„åˆæŸå¤±å‡½æ•°ï¼šç»“åˆå¤šç§æŸå¤±å‡½æ•°"""

    def __init__(self, num_classes, feature_dim, use_focal=True, use_label_smoothing=True,
                 use_center_loss=True, focal_alpha=0.25, focal_gamma=2.0,
                 label_smoothing_factor=0.1, center_loss_weight=0.003,
                 class_weights=None):
        super(CombinedLoss, self).__init__()

        self.use_focal = use_focal
        self.use_label_smoothing = use_label_smoothing
        self.use_center_loss = use_center_loss

        # åˆå§‹åŒ–å„ç§æŸå¤±å‡½æ•°
        if use_focal:
            self.focal_loss = FocalLoss(alpha=focal_alpha, gamma=focal_gamma, class_weights=class_weights)
        else:
            if class_weights is not None:
                self.ce_loss = nn.CrossEntropyLoss(weight=class_weights)
            else:
                self.ce_loss = nn.CrossEntropyLoss()

        if use_label_smoothing:
            self.label_smoothing_loss = LabelSmoothingCrossEntropy(smoothing=label_smoothing_factor)

        if use_center_loss:
            self.center_loss = CenterLoss(num_classes, feature_dim)

        self.center_loss_weight = center_loss_weight

    def forward(self, logits, features, targets):
        total_loss = 0
        loss_dict = {}

        # ä¸»è¦åˆ†ç±»æŸå¤±
        if self.use_focal:
            if self.use_label_smoothing:
                main_loss = self.label_smoothing_loss(logits, targets)
            else:
                main_loss = self.focal_loss(logits, targets)
        else:
            if self.use_label_smoothing:
                main_loss = self.label_smoothing_loss(logits, targets)
            else:
                main_loss = self.ce_loss(logits, targets)

        total_loss += main_loss
        loss_dict['main_loss'] = main_loss.item()

        # Center Loss
        if self.use_center_loss:
            center_loss = self.center_loss(features, targets)
            total_loss += self.center_loss_weight * center_loss
            loss_dict['center_loss'] = center_loss.item()

        return total_loss, loss_dict


# ==================== Pæ¨¡å‹å®šä¹‰ ====================
class FullPModel(nn.Module):
    """
    è¿™æ˜¯ä¸€ä¸ªåŒåˆ†æ”¯çš„ç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œä¸“é—¨è®¾è®¡ç”¨äºå¤„ç†é¢‘è°±å’ŒåŒ…ç»œä¸¤ç§ç‰¹å¾ï¼Œé€‚ç”¨äºéŸ³é¢‘ä¿¡å·åˆ†ç±»ä»»åŠ¡
    é¢‘è°±ç‰¹å¾ï¼ˆspecï¼‰ï¼š(B, 7, 48)  åŒ…ç»œç‰¹å¾ï¼ˆenvï¼‰ï¼š(B, 5, 32)
    """

    def __init__(self, n_spec_bands, n_env_centers, signal_length, num_classes=6, dropout=0.4):
        # signal_length åœ¨æ­¤æ¨¡å‹ä¸­æœªä½¿ç”¨ï¼Œä½†ä¿ç•™ç­¾åä»¥å…¼å®¹
        super().__init__()

        # é¢‘è°±ç‰¹å¾åˆ†æ”¯
        self.spec_branch = nn.Sequential(
            nn.Conv1d(n_spec_bands, 16, kernel_size=3, padding=1),
            nn.BatchNorm1d(16),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Conv1d(16, 32, kernel_size=3, padding=1),
            nn.BatchNorm1d(32),
            nn.ReLU(),
            nn.AdaptiveAvgPool1d(8),
            nn.Dropout(dropout)
        )
        """
        è¾“å…¥: [batch, n_spec_bands (7), spec_length (48)]
            â†’ Conv1d â†’ [batch, 16, 48] 
            â†’ Conv1d â†’ [batch, 32, 48]
            â†’ AdaptiveAvgPool1d â†’ [batch, 32, 8]
            â†’ view â†’ [batch, 32Ã—8 = 256]
        """

        # åŒ…ç»œç‰¹å¾åˆ†æ”¯
        self.env_branch = nn.Sequential(
            nn.Conv1d(n_env_centers, 16, kernel_size=3, padding=1),
            nn.BatchNorm1d(16),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Conv1d(16, 32, kernel_size=3, padding=1),
            nn.BatchNorm1d(32),
            nn.ReLU(),
            nn.AdaptiveAvgPool1d(8),
            nn.Dropout(dropout)
        )
        """
        è¾“å…¥: [batch, n_env_centers (5), env_length (32)]
            â†’ Conv1d â†’ [batch, 16, 32]
            â†’ Conv1d â†’ [batch, 32, 32]  
            â†’ AdaptiveAvgPool1d â†’ [batch, 32, 8]
            â†’ view â†’ [batch, 32Ã—8 = 256]
        """

        # ç‰¹å¾èåˆ
        self.feature_fusion = nn.Sequential(
            nn.Linear(512, 128),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Dropout(dropout)
        )
        """
        é¢‘è°±ç‰¹å¾: [batch, 256]
        åŒ…ç»œç‰¹å¾: [batch, 256]
        æ‹¼æ¥: [batch, 512]
            â†’ Linear â†’ [batch, 128]
            â†’ Linear â†’ [batch, 64]  # æœ€ç»ˆç‰¹å¾å‘é‡
        """

        # åˆ†ç±»å™¨
        self.classifier = nn.Linear(64, num_classes)

    def forward(self, spec, env, return_features=False):
        """
        spec: é¢‘è°±æ•°æ®ï¼Œå½¢çŠ¶ [B, 7, 48]
        env: åŒ…ç»œæ•°æ®ï¼Œå½¢çŠ¶ [B, 5, 32]
        """
        # é¢‘è°±åˆ†æ”¯å¤„ç†
        spec_out = self.spec_branch(spec)  # è¾“å…¥: [B, 7, 48]
        spec_out = spec_out.view(spec.size(0), -1)  # è¾“å‡º: [B, 256]
        """
        spec_out æ˜¯ç»è¿‡é¢‘è°±åˆ†æ”¯å¤„ç†åçš„ç‰¹å¾å¼ é‡
        è¾“å…¥: spec = [B, 7, 48]
            â†“ ç»è¿‡spec_branchçš„æ¯ä¸€å±‚
        ç¬¬1å±‚Conv1d: [B, 7, 48] â†’ [B, 16, 48]
        ç¬¬2å±‚Conv1d: [B, 16, 48] â†’ [B, 32, 48]
        AdaptiveAvgPool1d: [B, 32, 48] â†’ [B, 32, 8]
        è¾“å‡º: spec_out = [B, 32, 8]

        view() æ˜¯PyTorchä¸­ç”¨äºæ”¹å˜å¼ é‡å½¢çŠ¶çš„å‡½æ•°
        spec.size(0) = Bï¼ˆæ‰¹é‡å¤§å°ï¼‰
        -1 = "è‡ªåŠ¨è®¡ç®—è¿™ä¸ªç»´åº¦çš„å¤§å°"
        åŸå§‹spec_outå½¢çŠ¶: [B, 32, 8]
        æƒ³è¦çš„æ–°å½¢çŠ¶: [B, ?]
        è‡ªåŠ¨è®¡ç®—: 32 Ã— 8 = 256
        æ‰€ä»¥: [B, 32, 8] â†’ [B, 256]
        """

        # åŒ…ç»œåˆ†æ”¯å¤„ç†
        env_out = self.env_branch(env)  # è¾“å…¥: [B, 5, 32]
        env_out = env_out.view(env.size(0), -1)  # è¾“å‡º: [B, 256]

        # ç‰¹å¾æ‹¼æ¥
        combined = torch.cat([spec_out, env_out], dim=1)  # è¾“å‡º: [B, 512]
        """
        æ ·æœ¬1: [é¢‘è°±256ä¸ªå€¼] + [åŒ…ç»œ256ä¸ªå€¼] = [512ä¸ªå€¼]
        ...
        æ ·æœ¬B: [é¢‘è°±256ä¸ªå€¼] + [åŒ…ç»œ256ä¸ªå€¼] = [512ä¸ªå€¼]
        """

        # ç‰¹å¾èåˆ
        features = self.feature_fusion(combined)  # è¾“å‡º: [B, 64]

        # åˆ†ç±»
        logits = self.classifier(features)  # è¾“å‡º: [B, num_classes]

        """
        å·ç§¯æ“ä½œï¼š
        æ—¶é—´ç‚¹:   1   2   3
        é¢‘å¸¦1: [â—‹, â—‹, â—‹] Ã— [wâ‚â‚, wâ‚â‚‚, wâ‚â‚ƒ]
        é¢‘å¸¦2: [â—‹, â—‹, â—‹] Ã— [wâ‚‚â‚, wâ‚‚â‚‚, wâ‚‚â‚ƒ]
        ...
        é¢‘å¸¦7: [â—‹, â—‹, â—‹] Ã— [wâ‚‡â‚, wâ‚‡â‚‚, wâ‚‡â‚ƒ]
        è¾“å‡º = (æ‰€æœ‰ä¹˜ç§¯ä¹‹å’Œ) + åç½®

        ç„¶åæ»‘åŠ¨åˆ°ä¸‹ä¸€ä¸ªä½ç½®

        æ—¶é—´ç‚¹:   2   3   4
        é¢‘å¸¦1: [â—‹, â—‹, â—‹] Ã— [wâ‚â‚, wâ‚â‚‚, wâ‚â‚ƒ]
        é¢‘å¸¦2: [â—‹, â—‹, â—‹] Ã— [wâ‚‚â‚, wâ‚‚â‚‚, wâ‚‚â‚ƒ]
        ...
        é¢‘å¸¦7: [â—‹, â—‹, â—‹] Ã— [wâ‚‡â‚, wâ‚‡â‚‚, wâ‚‡â‚ƒ]
        è¾“å‡º = (æ‰€æœ‰ä¹˜ç§¯ä¹‹å’Œ) + åç½®

        è¾“å‡ºé€šé“æ•°é‡æ˜¯16ï¼Œé‚£ä¹ˆå°±æœ‰16ä¸ªå·ç§¯æ ¸å»åšç‰¹å¾æå–
        è¾“å‡ºé•¿åº¦ = (è¾“å…¥é•¿åº¦ + 2Ã—padding - kernel_size) / stride + 1ï¼Œstrideé»˜è®¤æ˜¯1
        padding = (kernel_size - 1) // 2 ä¿æŒè¾“å…¥è¾“å‡ºé•¿åº¦ç›¸åŒ
        """

        if return_features:
            return logits, features
        else:
            return logits


# ==================== Optunaä¼˜åŒ–å‡½æ•°ï¼ˆ*** ä¿®æ”¹ç‰ˆ ***ï¼‰====================
def objective_cv(trial, augmented_folds, n_spec_bands, n_env_centers,
                 num_classes, device, optimization_dir):
    """
    *** ä¿®æ”¹ç‰ˆ ***
    Optunaä¼˜åŒ–ç›®æ ‡å‡½æ•°
    ä¸å†ä½¿ç”¨ StratifiedKFoldï¼Œè€Œæ˜¯éå†ä¼ å…¥çš„ augmented_folds åˆ—è¡¨
    """

    # è¶…å‚æ•°æœç´¢ç©ºé—´
    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)
    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64, 128])
    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.6)
    weight_decay = trial.suggest_float('weight_decay', 1e-6, 1e-2, log=True)

    # ä¼˜åŒ–å™¨é€‰æ‹©
    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'AdamW', 'SGD'])

    # å­¦ä¹ ç‡è°ƒåº¦å™¨å‚æ•°
    scheduler_factor = trial.suggest_float('scheduler_factor', 0.3, 0.8)
    scheduler_patience = trial.suggest_int('scheduler_patience', 5, 20)

    # æ¢¯åº¦è£å‰ª
    max_grad_norm = trial.suggest_float('max_grad_norm', 0.5, 2.0)

    # æ•°æ®å¢å¼ºå‚æ•°
    noise_factor = trial.suggest_float('noise_factor', 0.005, 0.05)
    scale_factor = trial.suggest_float('scale_factor', 0.05, 0.3)

    # æŸå¤±å‡½æ•°é€‰æ‹©
    use_focal_loss = trial.suggest_categorical('use_focal_loss', [True, False])
    use_label_smoothing = trial.suggest_categorical('use_label_smoothing', [True, False])
    use_center_loss = trial.suggest_categorical('use_center_loss', [True, False])

    # æŸå¤±å‡½æ•°å‚æ•°
    focal_alpha = trial.suggest_float('focal_alpha', 0.1, 0.5) if use_focal_loss else 0.25
    focal_gamma = trial.suggest_float('focal_gamma', 1.0, 3.0) if use_focal_loss else 2.0
    label_smoothing_factor = trial.suggest_float('label_smoothing_factor', 0.05, 0.2) if use_label_smoothing else 0.1
    center_loss_weight = trial.suggest_float('center_loss_weight', 0.001, 0.01, log=True) if use_center_loss else 0.003

    try:
        # *** ä¿®æ”¹ç‚¹: ä¸å†ä½¿ç”¨ SKFï¼Œè€Œæ˜¯éå†ä¼ å…¥çš„ folds ***
        fold_accuracies = []
        best_fold_acc = 0
        best_model_state = None
        n_folds = len(augmented_folds)

        print(f"\n  Trial {trial.number}: å¼€å§‹ {n_folds} æŠ˜äº¤å‰éªŒè¯ (ä½¿ç”¨é¢„åˆ’åˆ†æ•°æ®)...")

        for fold_idx, fold_data in enumerate(augmented_folds):
            # *** ä¿®æ”¹ç‚¹: ç›´æ¥ä» fold_data åŠ è½½æ•°æ® ***
            X_spec_train = fold_data['train_spec']
            X_env_train = fold_data['train_env']
            y_train = fold_data['train_labels']

            X_spec_val = fold_data['val_spec']
            X_env_val = fold_data['val_env']
            y_val = fold_data['val_labels']

            # å½’ä¸€åŒ– (åŸºäºå½“å‰æŠ˜çš„è®­ç»ƒæ•°æ®)
            spec_mean = X_spec_train.mean(axis=(0, 2), keepdims=True)
            spec_std = X_spec_train.std(axis=(0, 2), keepdims=True) + 1e-8
            X_spec_train_norm = (X_spec_train - spec_mean) / spec_std
            X_spec_val_norm = (X_spec_val - spec_mean) / spec_std

            env_mean = X_env_train.mean(axis=(0, 2), keepdims=True)
            env_std = X_env_train.std(axis=(0, 2), keepdims=True) + 1e-8
            X_env_train_norm = (X_env_train - env_mean) / env_std
            X_env_val_norm = (X_env_val - env_mean) / env_std

            # è®¡ç®—å½“å‰æŠ˜çš„ç±»åˆ«æƒé‡
            class_weights = calculate_class_weights(y_train)

            # åˆ›å»ºæ¨¡å‹
            model = FullPModel(
                n_spec_bands=n_spec_bands,
                n_env_centers=n_env_centers,
                signal_length=0,  # ä¸é‡è¦
                num_classes=num_classes,
                dropout=dropout_rate
            ).to(device)

            # åˆ›å»ºæ•°æ®åŠ è½½å™¨
            train_dataset = TensorDataset(
                torch.FloatTensor(X_spec_train_norm),
                torch.FloatTensor(X_env_train_norm),
                torch.LongTensor(y_train)
            )
            val_dataset = TensorDataset(
                torch.FloatTensor(X_spec_val_norm),
                torch.FloatTensor(X_env_val_norm),
                torch.LongTensor(y_val)
            )

            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
            val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

            # åˆ›å»ºæŸå¤±å‡½æ•°
            criterion = CombinedLoss(
                num_classes=num_classes,
                feature_dim=64,  # FullPModelçš„æœ€ç»ˆç‰¹å¾ç»´åº¦
                use_focal=use_focal_loss,
                use_label_smoothing=use_label_smoothing,
                use_center_loss=use_center_loss,
                focal_alpha=focal_alpha,
                focal_gamma=focal_gamma,
                label_smoothing_factor=label_smoothing_factor,
                center_loss_weight=center_loss_weight,
                class_weights=class_weights.to(device) if torch.cuda.is_available() else class_weights
            )

            # åˆ›å»ºä¼˜åŒ–å™¨
            if optimizer_name == 'Adam':
                optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)
            elif optimizer_name == 'AdamW':
                optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)
            else:  # SGD
                optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay=weight_decay, momentum=0.9)

            scheduler = optim.lr_scheduler.ReduceLROnPlateau(
                optimizer, mode='min', factor=scheduler_factor, patience=scheduler_patience
            )

            # è®­ç»ƒå½“å‰æŠ˜
            fold_best_acc = 0
            # *** ä¿®æ”¹ç‚¹: è°ƒä¼˜é˜¶æ®µä½¿ç”¨æ›´å°‘çš„ Epochs, e.g., 50 ***
            # (åŸä»£ç ä¸º min(50, NUM_EPOCHS), ä¿æŒä¸€è‡´)
            optuna_epochs = min(50, NUM_EPOCHS)
            for epoch in range(optuna_epochs):
                # è®­ç»ƒé˜¶æ®µ
                model.train()
                for spec_batch, env_batch, y_batch in train_loader:
                    spec_batch = spec_batch.to(device)
                    env_batch = env_batch.to(device)
                    y_batch = y_batch.to(device)

                    # æ•°æ®å¢å¼º
                    if np.random.random() < 0.5:
                        spec_batch = spec_batch + torch.randn_like(spec_batch) * noise_factor
                        env_batch = env_batch + torch.randn_like(env_batch) * noise_factor
                    if np.random.random() < 0.3:
                        scale = torch.rand(spec_batch.size(0), 1, 1).to(device) * scale_factor + (1 - scale_factor / 2)
                        spec_batch = spec_batch * scale
                        env_batch = env_batch * scale

                    optimizer.zero_grad()
                    outputs, features = model(spec_batch, env_batch, return_features=True)
                    loss, _ = criterion(outputs, features, y_batch)
                    loss.backward()

                    # æ¢¯åº¦è£å‰ª
                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=max_grad_norm)

                    optimizer.step()

                # éªŒè¯é˜¶æ®µ
                model.eval()
                val_loss = 0
                val_correct = 0
                val_total = 0

                with torch.no_grad():
                    for spec_batch, env_batch, y_batch in val_loader:
                        spec_batch = spec_batch.to(device)
                        env_batch = env_batch.to(device)
                        y_batch = y_batch.to(device)

                        outputs, features = model(spec_batch, env_batch, return_features=True)
                        loss, _ = criterion(outputs, features, y_batch)

                        val_loss += loss.item()
                        _, predicted = torch.max(outputs.data, 1)
                        val_total += y_batch.size(0)
                        val_correct += (predicted == y_batch).sum().item()

                val_acc = 100 * val_correct / val_total
                scheduler.step(val_loss)

                if val_acc > fold_best_acc:
                    fold_best_acc = val_acc

            # æŠ¥å‘Šä¸­é—´ç»“æœ (ç”¨äº Pruning)
            trial.report(fold_best_acc, fold_idx)
            if trial.should_prune():
                print(f"    æŠ˜ {fold_idx + 1}/{n_folds}: {fold_best_acc:.2f}% (Trial Pruned)")
                raise optuna.exceptions.TrialPruned()

            fold_accuracies.append(fold_best_acc)
            print(f"    æŠ˜ {fold_idx + 1}/{n_folds}: {fold_best_acc:.2f}%")

            # ä¿å­˜æœ€ä½³æŠ˜çš„æ¨¡å‹
            if fold_best_acc > best_fold_acc:
                best_fold_acc = fold_best_acc
                best_model_state = model.state_dict().copy()

        # è®¡ç®—å¹³å‡å‡†ç¡®ç‡
        mean_acc = np.mean(fold_accuracies)
        std_acc = np.std(fold_accuracies)

        print(f"  Trial {trial.number} ç»“æœ: å¹³å‡={mean_acc:.2f}% (Â±{std_acc:.2f}%), æœ€ä½³={best_fold_acc:.2f}%")

        # ä¿å­˜æœ€ä½³æŠ˜çš„æ¨¡å‹
        trial_model_path = f"{optimization_dir}/trial_{trial.number}_model.pth"
        if best_model_state:
            torch.save(best_model_state, trial_model_path)

        return mean_acc

    except Exception as e:
        print(f"Trial {trial.number} failed with error: {e}")
        import traceback
        traceback.print_exc()
        return 0.0


def hyperparameter_optimization(augmented_folds, num_classes, device):
    """
    *** ä¿®æ”¹ç‰ˆ ***
    è¶…å‚æ•°ä¼˜åŒ–ä¸»å‡½æ•°
    æ¥æ”¶ augmented_folds åˆ—è¡¨
    """
    print("\n" + "=" * 60)
    print("ğŸ” å¼€å§‹è¶…å‚æ•°ä¼˜åŒ–ï¼ˆä½¿ç”¨é¢„åˆ’åˆ†çš„5æŠ˜æ•°æ®ï¼‰")
    print("=" * 60)

    # *** ä¿®æ”¹ç‚¹: ä» augmented_folds è·å–æ•°æ®ä¿¡æ¯ ***
    # åŠ è½½æ•°æ®ï¼ˆä½¿ç”¨å…¨éƒ¨è®­ç»ƒæ•°æ®è¿›è¡Œäº¤å‰éªŒè¯ï¼‰
    try:
        sample_fold = augmented_folds[0]
        sample_spec = sample_fold['train_spec']
        sample_env = sample_fold['train_env']
        n_folds = len(augmented_folds)
    except (IndexError, KeyError) as e:
        print(f"é”™è¯¯: 'augmented_folds' ç»“æ„ä¸æ­£ç¡®æˆ–ä¸ºç©º: {e}")
        return None, ""

    # æ¨¡å‹å‚æ•°
    n_spec_bands = sample_spec.shape[1]
    n_env_centers = sample_env.shape[1]
    # signal_length ä¸å†éœ€è¦

    print(f"\næ•°æ®é›†ä¿¡æ¯:")
    print(f"  äº¤å‰éªŒè¯æŠ˜æ•°: {n_folds} (æ¥è‡ª pkl æ–‡ä»¶)")
    print(f"  é¢‘è°±é€šé“æ•°: {n_spec_bands}")
    print(f"  åŒ…ç»œé€šé“æ•°: {n_env_centers}")
    print(f"  ç±»åˆ«æ•°: {num_classes}")

    # åˆ›å»ºä¼˜åŒ–ç»“æœä¿å­˜ç›®å½•
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    # *** ä¿®æ”¹ç‚¹: ä½¿ç”¨ç›¸å¯¹è·¯å¾„ ***
    optimization_dir = f"./hyperparameter_optimization_{timestamp}"
    Path(optimization_dir).mkdir(parents=True, exist_ok=True)

    # åˆ›å»ºOptuna study
    study = optuna.create_study(
        direction='maximize',
        sampler=optuna.samplers.TPESampler(seed=42),
        pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=10)
    )

    # å…¨å±€æœ€ä½³æ¨¡å‹è¿½è¸ª
    best_trial_number = [None]  # ä½¿ç”¨åˆ—è¡¨ä»¥ä¾¿åœ¨å›è°ƒå‡½æ•°ä¸­ä¿®æ”¹

    # å®šä¹‰å›è°ƒå‡½æ•°ï¼šè¿½è¸ªå¹¶ä¿å­˜å…¨å±€æœ€ä½³æ¨¡å‹
    def save_best_model_callback(study, trial):
        if study.best_trial.number == trial.number:
            # å½“å‰è¯•éªŒæ˜¯æ–°çš„æœ€ä½³è¯•éªŒ
            best_trial_number[0] = trial.number
            trial_model_path = f"{optimization_dir}/trial_{trial.number}_model.pth"
            best_model_path = f"{optimization_dir}/best_model.pth"

            # å¤åˆ¶å½“å‰è¯•éªŒçš„æ¨¡å‹ä¸ºå…¨å±€æœ€ä½³æ¨¡å‹
            if os.path.exists(trial_model_path):
                shutil.copy2(trial_model_path, best_model_path)
                print(f"\nâœ¨ æ–°çš„æœ€ä½³è¯•éªŒ #{trial.number}: éªŒè¯å‡†ç¡®ç‡ = {trial.value:.2f}%")

    # å®šä¹‰ç›®æ ‡å‡½æ•°
    def objective_wrapper(trial):
        # *** ä¿®æ”¹ç‚¹: ä¼ é€’ augmented_folds åˆ—è¡¨ ***
        return objective_cv(
            trial, augmented_folds,
            n_spec_bands, n_env_centers, num_classes, device, optimization_dir
        )

    # å¼€å§‹ä¼˜åŒ–
    print(f"\nå¼€å§‹ä¼˜åŒ– (å…±{N_TRIALS}æ¬¡è¯•éªŒï¼Œæ¯æ¬¡{n_folds}æŠ˜äº¤å‰éªŒè¯)...")
    study.optimize(
        objective_wrapper,
        n_trials=N_TRIALS,
        timeout=OPTIMIZATION_TIMEOUT,
        show_progress_bar=True,
        callbacks=[save_best_model_callback]
    )

    print(f"\nâœ… ä¼˜åŒ–å®Œæˆ!")
    print(f"æœ€ä½³è¯•éªŒ: {study.best_trial.number}")
    print(f"æœ€ä½³å¹³å‡éªŒè¯å‡†ç¡®ç‡ï¼ˆ{n_folds}æŠ˜äº¤å‰éªŒè¯ï¼‰: {study.best_value:.2f}%")
    print(f"\næœ€ä½³å‚æ•°:")
    for key, value in study.best_params.items():
        print(f"  {key}: {value}")

    # ä¿å­˜æœ€ä½³å‚æ•°
    with open(f"{optimization_dir}/best_params.json", 'w') as f:
        json.dump(study.best_params, f, indent=2)

    # ä¿å­˜æ‰€æœ‰è¯•éªŒç»“æœ
    trials_data = []
    for trial in study.trials:
        trials_data.append({
            'number': trial.number,
            'value': trial.value,
            'params': trial.params,
            'state': trial.state.name
        })

    with open(f"{optimization_dir}/all_trials.json", 'w') as f:
        json.dump(trials_data, f, indent=2)

    # ä¿å­˜æ¨¡å‹æ¶æ„ä¿¡æ¯
    model_info = {
        'best_trial_number': study.best_trial.number,
        'best_accuracy': study.best_value,
        'optimization_method': f'{n_folds}-fold cross-validation (pre-folded)',
        'n_folds': n_folds,
        'n_spec_bands': n_spec_bands,
        'n_env_centers': n_env_centers,
        'num_classes': num_classes,
        'model_architecture': 'FullPModel'
    }

    with open(f"{optimization_dir}/model_info.json", 'w') as f:
        json.dump(model_info, f, indent=2)

    # ä¿å­˜ä¼˜åŒ–å†å²å›¾
    try:
        if len(study.trials) > 0:
            fig = optuna.visualization.plot_optimization_history(study)
            fig.write_image(f"{optimization_dir}/optimization_history.png")

            fig = optuna.visualization.plot_param_importances(study)
            fig.write_image(f"{optimization_dir}/param_importances.png")
    except Exception as e:
        print(f"æ— æ³•ç”Ÿæˆä¼˜åŒ–å¯è§†åŒ–å›¾è¡¨: {e}")

    # æ¸…ç†éæœ€ä½³è¯•éªŒçš„æ¨¡å‹æ–‡ä»¶ï¼ˆèŠ‚çœç©ºé—´ï¼‰
    print(f"\næ¸…ç†ä¸­é—´æ¨¡å‹æ–‡ä»¶...")
    cleaned_count = 0
    for trial in study.trials:
        if trial.number != study.best_trial.number:
            trial_model_path = f"{optimization_dir}/trial_{trial.number}_model.pth"
            if os.path.exists(trial_model_path):
                os.remove(trial_model_path)
                cleaned_count += 1

    print(f"å·²åˆ é™¤ {cleaned_count} ä¸ªéæœ€ä½³æ¨¡å‹æ–‡ä»¶")

    print(f"\nä¼˜åŒ–ç»“æœå·²ä¿å­˜åˆ°: {optimization_dir}")
    print(f"  - best_model.pth: å…¨å±€æœ€ä½³æ¨¡å‹æƒé‡ (è¯•éªŒ #{study.best_trial.number}, é¢„åˆ’åˆ† {n_folds} æŠ˜CVä¸­çš„æœ€ä½³æŠ˜)")
    print(f"  - best_params.json: æœ€ä½³è¶…å‚æ•°")
    print(f"  - model_info.json: æ¨¡å‹æ¶æ„ä¿¡æ¯")
    print(f"  - all_trials.json: æ‰€æœ‰è¯•éªŒè®°å½•")
    print(f"\nè¯´æ˜: æ¨¡å‹æƒé‡æ¥è‡ªæœ€ä½³è¯•éªŒä¸­å‡†ç¡®ç‡æœ€é«˜çš„æŠ˜")

    return study.best_params, optimization_dir


# ==================== è®­ç»ƒå‡½æ•° ====================
def train_model(dataset, num_classes, save_dir, best_params=None):
    """
    è®­ç»ƒæ¨¡å‹
    *** ä¿®æ”¹ç‚¹: æ¥æ”¶ num_classes, å¹¶åˆ›å»º id_to_label ***
    """
    print("\n" + "=" * 60)
    print("è®­ç»ƒæœ€ç»ˆæ¨¡å‹")
    print("=" * 60)

    # ä½¿ç”¨æœ€ä½³å‚æ•°æˆ–é»˜è®¤å‚æ•°
    if best_params:
        print("\nä½¿ç”¨ä¼˜åŒ–åçš„æœ€ä½³å‚æ•°:")
        for key, value in best_params.items():
            print(f"  {key}: {value}")

        learning_rate = best_params.get('learning_rate', LEARNING_RATE)
        batch_size = best_params.get('batch_size', BATCH_SIZE)
        dropout_rate = best_params.get('dropout_rate', DROPOUT_RATE)
        weight_decay = best_params.get('weight_decay', 0.0)
        optimizer_name = best_params.get('optimizer', 'Adam')
        scheduler_factor = best_params.get('scheduler_factor', 0.5)
        scheduler_patience = best_params.get('scheduler_patience', 10)
        max_grad_norm = best_params.get('max_grad_norm', 1.0)
        noise_factor = best_params.get('noise_factor', 0.01)
        scale_factor = best_params.get('scale_factor', 0.1)

        # æŸå¤±å‡½æ•°å‚æ•°
        use_focal_loss = best_params.get('use_focal_loss', False)
        use_label_smoothing = best_params.get('use_label_smoothing', False)
        use_center_loss = best_params.get('use_center_loss', False)
        focal_alpha = best_params.get('focal_alpha', 0.25)
        focal_gamma = best_params.get('focal_gamma', 2.0)
        label_smoothing_factor = best_params.get('label_smoothing_factor', 0.1)
        center_loss_weight = best_params.get('center_loss_weight', 0.003)
    else:
        print("\nä½¿ç”¨é»˜è®¤å‚æ•°")
        learning_rate = LEARNING_RATE
        batch_size = BATCH_SIZE
        dropout_rate = DROPOUT_RATE
        weight_decay = 0.0
        optimizer_name = 'Adam'
        scheduler_factor = 0.5
        scheduler_patience = 10
        max_grad_norm = 1.0
        noise_factor = 0.01
        scale_factor = 0.1

        use_focal_loss = False
        use_label_smoothing = False
        use_center_loss = False
        focal_alpha = 0.25
        focal_gamma = 2.0
        label_smoothing_factor = 0.1
        center_loss_weight = 0.003

    # åŠ è½½æ•°æ®
    spec_train_all = dataset['x_train']['spec']
    env_train_all = dataset['x_train']['env']
    labels_all = dataset['y_train']

    # *** ä¿®æ”¹ç‚¹: åˆ›å»º id_to_label ***
    id_to_label = dataset.get('id_to_label', {i: f'Class_{i}' for i in range(num_classes)})

    print(f"\næ•°æ®é›†ä¿¡æ¯:")
    print(f"  è®­ç»ƒé›†é¢‘è°±ç‰¹å¾: {spec_train_all.shape}")
    print(f"  è®­ç»ƒé›†åŒ…ç»œç‰¹å¾: {env_train_all.shape}")
    print(f"  è®­ç»ƒé›†æ ‡ç­¾: {labels_all.shape}")
    print(f"  ç±»åˆ«æ•°: {num_classes}")

    # åˆ’åˆ†è®­ç»ƒéªŒè¯é›†
    X_spec_train, X_spec_val, X_env_train, X_env_val, y_train, y_val = train_test_split(
        spec_train_all, env_train_all, labels_all, test_size=0.2, random_state=42, stratify=labels_all
    )

    print(f"\nè®­ç»ƒéªŒè¯é›†åˆ’åˆ† (80/20 split):")
    print(f"  è®­ç»ƒé›†: {len(y_train)} æ ·æœ¬")
    print(f"  éªŒè¯é›†: {len(y_val)} æ ·æœ¬")

    # æ ‡ç­¾åˆ†å¸ƒ
    print(f"\nè®­ç»ƒé›†æ ‡ç­¾åˆ†å¸ƒ:")
    for label_id in range(num_classes):
        count = np.sum(y_train == label_id)
        print(f"  ç±»åˆ« {label_id} ({id_to_label.get(label_id, 'N/A')}): {count} æ ·æœ¬")

    # å½’ä¸€åŒ–
    spec_mean = X_spec_train.mean(axis=(0, 2), keepdims=True)
    spec_std = X_spec_train.std(axis=(0, 2), keepdims=True) + 1e-8
    X_spec_train_norm = (X_spec_train - spec_mean) / spec_std
    X_spec_val_norm = (X_spec_val - spec_mean) / spec_std

    env_mean = X_env_train.mean(axis=(0, 2), keepdims=True)
    env_std = X_env_train.std(axis=(0, 2), keepdims=True) + 1e-8
    X_env_train_norm = (X_env_train - env_mean) / env_std
    X_env_val_norm = (X_env_val - env_mean) / env_std

    # ä¿å­˜å½’ä¸€åŒ–å‚æ•°
    norm_params = {
        'spec_mean': spec_mean,
        'spec_std': spec_std,
        'env_mean': env_mean,
        'env_std': env_std
    }

    # è®¡ç®—ç±»åˆ«æƒé‡
    class_weights = calculate_class_weights(y_train)

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_dataset = TensorDataset(
        torch.FloatTensor(X_spec_train_norm),
        torch.FloatTensor(X_env_train_norm),
        torch.LongTensor(y_train)
    )
    val_dataset = TensorDataset(
        torch.FloatTensor(X_spec_val_norm),
        torch.FloatTensor(X_env_val_norm),
        torch.LongTensor(y_val)
    )

    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

    # åˆ›å»ºæ¨¡å‹
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"\nä½¿ç”¨è®¾å¤‡: {device}")

    n_spec_bands = spec_train_all.shape[1]
    n_env_centers = env_train_all.shape[1]
    # signal_length = 0 # ä¸é‡è¦

    model = FullPModel(
        n_spec_bands=n_spec_bands,
        n_env_centers=n_env_centers,
        signal_length=0,  # ä¸é‡è¦
        num_classes=num_classes,
        dropout=dropout_rate
    ).to(device)

    print(f"\næ¨¡å‹æ¶æ„:")
    print(f"  é¢‘è°±é€šé“æ•°: {n_spec_bands}")
    print(f"  åŒ…ç»œé€šé“æ•°: {n_env_centers}")
    print(f"  Dropoutç‡: {dropout_rate}")

    # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = CombinedLoss(
        num_classes=num_classes,
        feature_dim=64,
        use_focal=use_focal_loss,
        use_label_smoothing=use_label_smoothing,
        use_center_loss=use_center_loss,
        focal_alpha=focal_alpha,
        focal_gamma=focal_gamma,
        label_smoothing_factor=label_smoothing_factor,
        center_loss_weight=center_loss_weight,
        class_weights=class_weights.to(device) if torch.cuda.is_available() else class_weights
    )

    if optimizer_name == 'Adam':
        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)
    elif optimizer_name == 'AdamW':
        optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)
    else:  # SGD
        optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay=weight_decay, momentum=0.9)

    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=scheduler_factor, patience=scheduler_patience)

    # è®­ç»ƒå¾ªç¯
    train_losses = []
    val_losses = []
    train_accs = []
    val_accs = []
    best_val_acc = 0
    patience_counter = 0

    print(f"\nå¼€å§‹è®­ç»ƒ (å…±{NUM_EPOCHS}è½®)...")
    print("=" * 60)

    for epoch in range(NUM_EPOCHS):
        # è®­ç»ƒ
        model.train()
        train_loss = 0
        train_correct = 0
        train_total = 0

        for spec_batch, env_batch, y_batch in train_loader:
            spec_batch, env_batch, y_batch = spec_batch.to(device), env_batch.to(device), y_batch.to(device)

            # æ•°æ®å¢å¼º
            if np.random.random() < 0.5:
                spec_batch = spec_batch + torch.randn_like(spec_batch) * noise_factor
                env_batch = env_batch + torch.randn_like(env_batch) * noise_factor
            if np.random.random() < 0.3:
                scale = torch.rand(spec_batch.size(0), 1, 1).to(device) * scale_factor + (1 - scale_factor / 2)
                spec_batch = spec_batch * scale
                env_batch = env_batch * scale

            optimizer.zero_grad()
            outputs, features = model(spec_batch, env_batch, return_features=True)
            loss, _ = criterion(outputs, features, y_batch)
            loss.backward()

            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=max_grad_norm)

            optimizer.step()

            train_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            train_total += y_batch.size(0)
            train_correct += (predicted == y_batch).sum().item()

        train_loss /= len(train_loader)
        train_acc = 100 * train_correct / train_total

        # éªŒè¯
        model.eval()
        val_loss = 0
        val_correct = 0
        val_total = 0

        with torch.no_grad():
            for spec_batch, env_batch, y_batch in val_loader:
                spec_batch, env_batch, y_batch = spec_batch.to(device), env_batch.to(device), y_batch.to(device)

                outputs, features = model(spec_batch, env_batch, return_features=True)
                loss, _ = criterion(outputs, features, y_batch)

                val_loss += loss.item()
                _, predicted = torch.max(outputs.data, 1)
                val_total += y_batch.size(0)
                val_correct += (predicted == y_batch).sum().item()

        val_loss /= len(val_loader)
        val_acc = 100 * val_correct / val_total

        train_losses.append(train_loss)
        val_losses.append(val_loss)
        train_accs.append(train_acc)
        val_accs.append(val_acc)

        # æ¯5è½®æ‰“å°ä¸€æ¬¡
        if (epoch + 1) % 5 == 0 or epoch == 0:
            print(f'Epoch [{epoch + 1:3d}/{NUM_EPOCHS}] '
                  f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, '
                  f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')

        scheduler.step(val_loss)

        # æ—©åœ
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            torch.save(model.state_dict(), f"{save_dir}/best_model.pth")
            patience_counter = 0
        else:
            patience_counter += 1

        if patience_counter >= EARLY_STOPPING_PATIENCE:
            print(f"\næ—©åœäºç¬¬ {epoch + 1} è½®")
            break

    print("=" * 60)

    # åŠ è½½æœ€ä½³æ¨¡å‹
    model.load_state_dict(torch.load(f"{save_dir}/best_model.pth"))

    # æœ€ç»ˆè¯„ä¼°
    model.eval()
    y_pred = []
    y_true = []

    with torch.no_grad():
        for spec_batch, env_batch, y_batch in val_loader:
            spec_batch, env_batch = spec_batch.to(device), env_batch.to(device)
            outputs = model(spec_batch, env_batch, return_features=False)
            _, predicted = torch.max(outputs.data, 1)
            y_pred.extend(predicted.cpu().numpy())
            y_true.extend(y_batch.numpy())

    y_pred = np.array(y_pred)
    y_true = np.array(y_true)

    # æ‰“å°ç»“æœ
    print(f"\n{'=' * 60}")
    print("è®­ç»ƒå®Œæˆï¼")
    print(f"{'=' * 60}")
    print(f"\næœ€ä½³éªŒè¯é›†å‡†ç¡®ç‡: {best_val_acc:.2f}%")

    print(f"\nåˆ†ç±»æŠ¥å‘Š:")
    class_names = [id_to_label.get(i, f'Class_{i}') for i in range(num_classes)]
    print(classification_report(y_true, y_pred, target_names=class_names))

    # ä¿å­˜ç»“æœ
    results = {
        'train_losses': train_losses,
        'val_losses': val_losses,
        'train_accs': train_accs,
        'val_accs': val_accs,
        'best_val_acc': best_val_acc,
        'norm_params': norm_params,
        'y_true': y_true,
        'y_pred': y_pred,
        'class_names': class_names
    }

    with open(f"{save_dir}/training_results.pkl", 'wb') as f:
        pickle.dump(results, f)

    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    try:
        fig, axes = plt.subplots(1, 2, figsize=(15, 5))

        axes[0].plot(train_losses, label='Train Loss', linewidth=2)
        axes[0].plot(val_losses, label='Val Loss', linewidth=2)
        axes[0].set_xlabel('Epoch', fontsize=12)
        axes[0].set_ylabel('Loss', fontsize=12)
        axes[0].set_title('Training and Validation Loss', fontsize=14)
        axes[0].legend(fontsize=11)
        axes[0].grid(True, alpha=0.3)

        axes[1].plot(train_accs, label='Train Acc', linewidth=2)
        axes[1].plot(val_accs, label='Val Acc', linewidth=2)
        axes[1].set_xlabel('Epoch', fontsize=12)
        axes[1].set_ylabel('Accuracy (%)', fontsize=12)
        axes[1].set_title('Training and Validation Accuracy', fontsize=14)
        axes[1].legend(fontsize=11)
        axes[1].grid(True, alpha=0.3)

        plt.tight_layout()
        plt.savefig(f"{save_dir}/training_history.png", dpi=300, bbox_inches='tight')
        plt.close()
    except Exception as e:
        print(f"æ— æ³•ç»˜åˆ¶è®­ç»ƒæ›²çº¿: {e}")

    # ç»˜åˆ¶æ··æ·†çŸ©é˜µ
    try:
        cm = confusion_matrix(y_true, y_pred)
        plt.figure(figsize=(10, 8))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                    xticklabels=class_names, yticklabels=class_names,
                    cbar_kws={'label': 'Count'})
        plt.title('Confusion Matrix', fontsize=16)
        plt.xlabel('Predicted Label', fontsize=13)
        plt.ylabel('True Label', fontsize=13)
        plt.tight_layout()
        plt.savefig(f"{save_dir}/confusion_matrix.png", dpi=300, bbox_inches='tight')
        plt.close()
    except Exception as e:
        print(f"æ— æ³•ç»˜åˆ¶æ··æ·†çŸ©é˜µ: {e}")

    print(f"\nç»“æœå·²ä¿å­˜åˆ°: {save_dir}")
    print(f"  - best_model.pth: æœ€ä½³æ¨¡å‹")
    print(f"  - training_results.pkl: è®­ç»ƒç»“æœ")
    print(f"  - training_history.png: è®­ç»ƒæ›²çº¿")
    print(f"  - confusion_matrix.png: æ··æ·†çŸ©é˜µ")

    # *** ä¿®æ”¹ç‚¹: è¿”å› norm_params ***
    return model, results, norm_params


# ==================== ä¸»å‡½æ•° ====================
def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ä½¿ç”¨ä¼˜åŒ–æ•°æ®é›†è®­ç»ƒæ¨¡å‹ (æ”¯æŒ5æŠ˜äº¤å‰éªŒè¯è¶…å‚æ•°ä¼˜åŒ–)")
    print("=" * 60)

    # æ£€æŸ¥æ•°æ®é›†è·¯å¾„
    if not OPTIMIZED_DATASET_PATH or not os.path.exists(OPTIMIZED_DATASET_PATH):
        print(f"\né”™è¯¯: è¯·è®¾ç½®æ­£ç¡®çš„æ•°æ®é›†è·¯å¾„")
        print(f"å½“å‰è·¯å¾„: {OPTIMIZED_DATASET_PATH}")
        print(f"è¯·ç¡®ä¿ 'augmented_dataset_5fold.pkl' ä½äºæ­¤è·¯å¾„")
        return None, None

    # åŠ è½½æ•°æ®é›†
    print(f"\nåŠ è½½æ•°æ®é›†: {OPTIMIZED_DATASET_PATH}")
    try:
        with open(OPTIMIZED_DATASET_PATH, 'rb') as f:
            data = pickle.load(f)
    except Exception as e:
        print(f"åŠ è½½ pkl æ–‡ä»¶å¤±è´¥: {e}")
        return None, None

    # --- *** ä¿®æ”¹ç‚¹: ä»æ–° 'data' ç»“æ„ä¸­æå–æ•°æ® *** ---
    try:
        augmented_folds = data['augmented_folds']
        x_test_data = data['x_test']
        num_classes = data['num_classes']
        n_folds = data['n_splits']

        print(f"\nå·²ä» '{OPTIMIZED_DATASET_PATH}' åŠ è½½æ•°æ®:")
        print(f"  å‘ç° {n_folds} æŠ˜é¢„åˆ’åˆ†æ•°æ®ç”¨äºè°ƒä¼˜ã€‚")
        print(f"  å‘ç° {len(x_test_data['spec'])} æ¡å…¨å±€æµ‹è¯•æ ·æœ¬ã€‚")
        print(f"  ç±»åˆ«æ•°: {num_classes}")

    except (KeyError, IndexError, TypeError) as e:
        print(f"\né”™è¯¯: 'augmented_dataset_5fold.pkl' æ–‡ä»¶ç»“æ„ä¸æ­£ç¡®ã€‚")
        print(f"  éœ€è¦ 'augmented_folds' (åˆ—è¡¨), 'x_test' (å­—å…¸), 'num_classes' (æ•´æ•°).")
        print(f"  é”™è¯¯è¯¦æƒ…: {e}")
        return None, None
    # --- *** ä¿®æ”¹ç‚¹ç»“æŸ *** ---

    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"\nä½¿ç”¨è®¾å¤‡: {device}")

    # è¶…å‚æ•°ä¼˜åŒ–
    best_params = None
    optimization_dir = ""  # åˆå§‹åŒ–
    if ENABLE_HYPERPARAMETER_TUNING:
        print(f"\nå¯ç”¨è¶…å‚æ•°ä¼˜åŒ–...")
        # *** ä¿®æ”¹ç‚¹: ä¼ é€’ augmented_folds åˆ—è¡¨ ***
        best_params, optimization_dir = hyperparameter_optimization(augmented_folds, num_classes, device)
        print(f"\nè¶…å‚æ•°ä¼˜åŒ–å®Œæˆï¼Œæœ€ä½³å‚æ•°:")
        for key, value in best_params.items():
            print(f"  {key}: {value}")
    else:
        print(f"\nè·³è¿‡è¶…å‚æ•°ä¼˜åŒ–ï¼Œä½¿ç”¨é»˜è®¤å‚æ•°")

    # åˆ›å»ºä¿å­˜ç›®å½•
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    # *** ä¿®æ”¹ç‚¹: ä½¿ç”¨ç›¸å¯¹è·¯å¾„ ***
    save_dir = f"./training_results_{timestamp}"
    Path(save_dir).mkdir(parents=True, exist_ok=True)

    # ä¿å­˜æœ€ä½³å‚æ•°ï¼ˆå¦‚æœæœ‰ï¼‰
    if best_params:
        with open(f"{save_dir}/best_hyperparams.json", 'w') as f:
            json.dump(best_params, f, indent=2)
        print(f"\næœ€ä½³è¶…å‚æ•°å·²ä¿å­˜: {save_dir}/best_hyperparams.json")

    # --- *** ä¿®æ”¹ç‚¹: å‡†å¤‡ 'train_model' çš„æ•°æ® *** ---
    # æˆ‘ä»¬å°†ä½¿ç”¨ç¬¬1æŠ˜(ç´¢å¼•0)çš„å¢å¼ºæ•°æ®ä½œä¸º "æ€»è®­ç»ƒé›†"
    # 'train_model' å°†åœ¨è¿™ä¸ªæ•°æ®ä¸Šè¿›è¡Œè‡ªå·±çš„ 80/20 åˆ’åˆ†
    print(f"\nå‡†å¤‡ 'train_model' çš„æ•°æ® (ä½¿ç”¨ç¬¬ 1 æŠ˜çš„å¢å¼ºæ•°æ®)...")
    fold_1_data = augmented_folds[0]

    # é‡æ–°æ„å»º 'dataset' å­—å…¸ï¼Œä»¥é€‚åº” 'train_model' å‡½æ•°çš„æ ¼å¼
    dataset_for_final_train = {
        'x_train': {'spec': fold_1_data['train_spec'], 'env': fold_1_data['train_env']},
        'y_train': fold_1_data['train_labels'],
        # åˆ›å»ºæ ‡ç­¾æ˜ å°„
        'id_to_label': {i: f'Class_{i}' for i in range(num_classes)},
        'label_map': {f'Class_{i}': i for i in range(num_classes)}
    }

    # è®­ç»ƒæœ€ç»ˆæ¨¡å‹
    model, results, norm_params = train_model(dataset_for_final_train, num_classes, save_dir, best_params)

    # --- *** æ–°å¢: åœ¨å…¨å±€æµ‹è¯•é›†ä¸Šè¿›è¡Œé¢„æµ‹ *** ---
    print(f"\n{'=' * 60}")
    print("ğŸš€ å¼€å§‹åœ¨å…¨å±€æµ‹è¯•é›†ä¸Šè¿›è¡Œæœ€ç»ˆé¢„æµ‹...")
    print(f"{'=' * 60}")

    # åŠ è½½ 'train_model' ä¿å­˜çš„æœ€ä½³æ¨¡å‹
    best_model_path = f"{save_dir}/best_model.pth"
    if not os.path.exists(best_model_path):
        print(f"é”™è¯¯: æ‰¾ä¸åˆ°è®­ç»ƒå¥½çš„æ¨¡å‹ {best_model_path}")
        return model, results

    # å½’ä¸€åŒ–å‚æ•°å·²ä» train_model è¿”å›
    spec_mean = norm_params['spec_mean']
    spec_std = norm_params['spec_std']
    env_mean = norm_params['env_mean']
    env_std = norm_params['env_std']

    # åŠ è½½æµ‹è¯•æ•°æ®
    x_test_spec = data['x_test']['spec']
    x_test_env = data['x_test']['env']

    # å½’ä¸€åŒ–æµ‹è¯•æ•°æ®
    x_test_spec_norm = (x_test_spec - spec_mean) / spec_std
    x_test_env_norm = (x_test_env - env_mean) / env_std

    # åˆ›å»ºæµ‹è¯•æ•°æ®åŠ è½½å™¨
    test_dataset = TensorDataset(
        torch.FloatTensor(x_test_spec_norm),
        torch.FloatTensor(x_test_env_norm)
    )
    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)

    # åŠ è½½æ¨¡å‹
    n_spec_bands = dataset_for_final_train['x_train']['spec'].shape[1]
    n_env_centers = dataset_for_final_train['x_train']['env'].shape[1]

    # ä½¿ç”¨ 'best_params' (å¦‚æœå­˜åœ¨)
    final_dropout = best_params.get('dropout_rate', DROPOUT_RATE) if best_params else DROPOUT_RATE

    model = FullPModel(
        n_spec_bands=n_spec_bands,
        n_env_centers=n_env_centers,
        signal_length=0,  # ä¸é‡è¦
        num_classes=num_classes,
        dropout=final_dropout
    ).to(device)

    model.load_state_dict(torch.load(best_model_path))
    model.eval()
    print("å·²æˆåŠŸåŠ è½½æœ€ä½³æ¨¡å‹å’Œå½’ä¸€åŒ–å‚æ•°ã€‚")

    # å¼€å§‹é¢„æµ‹
    test_preds = []
    with torch.no_grad():
        for spec_batch, env_batch in test_loader:
            spec_batch = spec_batch.to(device)
            env_batch = env_batch.to(device)

            outputs = model(spec_batch, env_batch, return_features=False)
            _, predicted = torch.max(outputs.data, 1)
            test_preds.extend(predicted.cpu().numpy())

    test_preds = np.array(test_preds)

    # ä¿å­˜é¢„æµ‹ç»“æœ
    pred_save_path = f"{save_dir}/test_predictions.npy"
    np.save(pred_save_path, test_preds)

    # åŒæ—¶ä¿å­˜ä¸º .txt (å¦‚æœéœ€è¦)
    pred_txt_path = f"{save_dir}/test_predictions.txt"
    np.savetxt(pred_txt_path, test_preds, fmt='%d')

    print(f"\né¢„æµ‹å®Œæˆ! {len(test_preds)} æ¡é¢„æµ‹ç»“æœå·²ä¿å­˜ã€‚")
    print(f"  - {pred_save_path}")
    print(f"  - {pred_txt_path}")

    # æ‰“å°ä¸€äº›é¢„æµ‹ç¤ºä¾‹
    print(f"\né¢„æµ‹ç»“æœç¤ºä¾‹ (å‰20æ¡):")
    print(test_preds[:20])

    print(f"\n{'=' * 60}")
    print("âœ… ä»»åŠ¡å…¨éƒ¨å®Œæˆ!")
    print(f"{'=' * 60}")
    print(f"\næœ€ç»ˆæ¨¡å‹å’Œè®­ç»ƒç»“æœä¿å­˜ä½ç½®: {save_dir}")
    if best_params:
        print(f"è¶…å‚æ•°ä¼˜åŒ–ç»“æœ (åŸºäºé¢„åˆ’åˆ†5æŠ˜æ•°æ®): {optimization_dir}")

    return model, results


if __name__ == "__main__":
    model, results = main()